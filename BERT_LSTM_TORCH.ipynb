{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# This will make only the GPUs with index 4 and 5 visible to PyTorch\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '5, 6'\n",
        "\n",
        "# Check if CUDA is available and then set the device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')  # This will use the first available GPU in the list, i.e., GPU 4\n",
        "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Using CPU')\n",
        "\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363,
          "referenced_widgets": [
            "56a0e6dec05f457ba62169de5d403816",
            "9fe771f29b644fe48389be1c9ae56670",
            "7824678ff8b640689489182b7fdf44db",
            "bb059786d42543ce9c9284766291e568",
            "cd9b8d9f37964e689938b8e9ffa31366",
            "655d961e6134473a95210a6b91bf9b9f",
            "83b4594b9629415984c2e0e12887564e",
            "874e6f689a894aeeb27c7dfa13d91099",
            "2626087347db4e58a8cd54feb2beeadd",
            "fec95d06874447a3a27a0753b5026043",
            "c412c019398a4570930da3fbf7b0a5be",
            "a3a7ecbac6674be9912d4e626b961108",
            "b99433b11a064605828d104a12c40f4a",
            "b130dd7c94ac403c922dd620741ad366",
            "2c450c32aa774c9ebe2bf35c73bf4eed",
            "b42c9283f69b4201815620fc5acdde2f",
            "0b7cbcccd47241c4b2ef7a33ee9dc5ea",
            "74284f8722474e41891e99edf21c0df2",
            "55f2073b5c424473bcb2277323549e3a",
            "03fb54651ae0420890eeb25a410becba",
            "eefbfad15ce540c4b9c63c86c48d5d9a",
            "2c60934f7821478aa0dac546dea2b57f",
            "28320915ba4344fe9a54071dfbb726d7",
            "7308d4b0adb34486bc5e4e58f0375d09",
            "74a5028ceb7c40efbc76e03d974ed070",
            "bffe7ba8f11642faa0e36dabc60c8836",
            "6e01005ae3a740fda7a006fb78ba8740",
            "5a3a9c81442d40deb9f6c0fd865d9411",
            "3bf7f64ccf94442294d478b69f09fcf0",
            "52d51502b6a14d1db2d6be60dfe0dc54",
            "1fef5586ac0b411dafe680263fcb287b",
            "7730ac7fad314ff08180154ff27de8a5",
            "bd2f8f636f8e40909bece34fc2bae374",
            "59c20e1ae2a847debda8a950d35a1704",
            "c705fa24849a4bac88104f28fc51d2d1",
            "28e56b6bd3424b1eba20ee274de2961c",
            "e15ab9dc43ab420e9ffed8f0d5df5951",
            "13c3fdb9819e48c682527ef715f7b087",
            "6d0113afd6e2436d8271e1be124c7151",
            "c2d78b2ca116425997345bfc24dfd4f6",
            "9d2ce78ef8294030b716d08151182393",
            "c4eb593500d643c9aba800b3a9d03be8",
            "b705ffe3c8004594aa1dbd4a0aab4288",
            "f62621a2f6e64d898f829fd71aab6646",
            "cdc8b98e49ff4d08a48e4868d4abe3c0",
            "47e2523eae8f4ba9bc0cc740b98fc84e",
            "ddb1fdd546cd409ebbc25f50def745b3",
            "9a94e22b799f4e2b86b183d0b2855abe",
            "eb23ef4a723a40b199e87edc74434198",
            "8cdad662934b4a8aa1df0c7b01d8fec8",
            "cdc177ddd3a448b4befa3f9d3452b01b",
            "132c2d61a02f4751bcd99870a7e284ea",
            "932b21d80dae4372ad6b5ac1421fa828",
            "3f5248e82da44eb8afcdfaf262f725ce",
            "be1cade7dac14367a7d165b92e23c1e0"
          ]
        },
        "id": "PkANHSTqXQWg",
        "outputId": "1d7e4bc7-a73d-4455-b1ed-8ca566691ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "df = pd.read_csv('/home/leili/Vul/LLMDatasets/Datasets/CSDatasets/formatted_Sample_Original_0_6370.csv')\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).to(device)\n",
        "#model = model.half()\n",
        "\n",
        "def batch_process(texts, layer_index):\n",
        "\n",
        "    tokens = tokenizer(texts, add_special_tokens=True, max_length=120, padding='max_length', truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "    layer_output = hidden_states[layer_index].detach().to('cpu')\n",
        "    return layer_output.numpy()\n",
        "\n",
        "batch_size = 100\n",
        "results = []\n",
        "\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch_texts = df['text'][i:i + batch_size].tolist()\n",
        "    batch_output = batch_process(batch_texts,12)\n",
        "    results.append(batch_output)\n",
        "\n",
        "results = np.vstack(results)\n",
        "\n",
        "\n",
        "expected_shape = (len(df), 120, 768)\n",
        "if results.shape == expected_shape:\n",
        "    df['layer_12_output'] = list(results)\n",
        "else:\n",
        "    print(f\"Unexpected result shape: {results.shape}. Expected: {expected_shape}\")\n",
        "\n",
        "df['av'] = df['av'].map({'N': 0, 'L': 1, 'A': 2, 'P': 3})  # Adjust mapping as needed\n",
        "df['ac'] = df['ac'].map({'L': 0, 'H': 1})\n",
        "df['pr'] = df['pr'].map({'L': 0, 'H': 1, 'N': 2})\n",
        "df['ui'] = df['ui'].map({'R': 0, 'N': 1})\n",
        "df['s'] = df['s'].map({'C': 0, 'U': 1})\n",
        "df['c'] = df['c'].map({'H': 0, 'N': 1, 'L': 2})\n",
        "df['i'] = df['i'].map({'H': 0, 'N': 1, 'L': 2})\n",
        "df['a'] = df['a'].map({'H': 0, 'N': 1, 'L': 2})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "daTqjBjOXcuf",
        "outputId": "0720e03f-ce94-45f8-8219-a1f821939045"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>vectorString</th>\n",
              "      <th>av</th>\n",
              "      <th>ac</th>\n",
              "      <th>pr</th>\n",
              "      <th>ui</th>\n",
              "      <th>s</th>\n",
              "      <th>c</th>\n",
              "      <th>i</th>\n",
              "      <th>a</th>\n",
              "      <th>score</th>\n",
              "      <th>baseSeverity</th>\n",
              "      <th>layer_12_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CVE-2016-0002</td>\n",
              "      <td>The Microsoft (1) VBScript 5.7 and 5.8 and (2)...</td>\n",
              "      <td>CVSS:3.0/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>HIGH</td>\n",
              "      <td>[[-0.18392727, -0.2513095, -0.39242408, -0.213...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CVE-2016-0003</td>\n",
              "      <td>Microsoft Edge allows remote attackers to exec...</td>\n",
              "      <td>CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>CRITICAL</td>\n",
              "      <td>[[-0.34586793, -0.48593235, -0.6668904, -0.174...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              ID                                               text  \\\n",
              "0  CVE-2016-0002  The Microsoft (1) VBScript 5.7 and 5.8 and (2)...   \n",
              "1  CVE-2016-0003  Microsoft Edge allows remote attackers to exec...   \n",
              "\n",
              "                                   vectorString  av  ac  pr  ui  s  c  i  a  \\\n",
              "0  CVSS:3.0/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H   0   1   2   0  1  0  0  0   \n",
              "1  CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H   0   0   2   0  0  0  0  0   \n",
              "\n",
              "   score baseSeverity                                    layer_12_output  \n",
              "0    7.5         HIGH  [[-0.18392727, -0.2513095, -0.39242408, -0.213...  \n",
              "1    9.6     CRITICAL  [[-0.34586793, -0.48593235, -0.6668904, -0.174...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ym5NQuFXjMb",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 8 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 246.50529834628105, Val Loss 54.76547414064407, Val Accuracy 0.7291993720565149, Precision 0.9322998430141287, Recall 0.25, F1 0.210848842487517, Cohen Kappa 0.0, Accuracy 0.7291993720565149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 193.4966683089733, Val Loss 44.26259544491768, Val Accuracy 0.7912087912087912, Precision 0.8682183124499556, Recall 0.34960724512266333, F1 0.35418442780337944, Cohen Kappa 0.41162601329590065, Accuracy 0.7912087912087912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 169.26673232018948, Val Loss 40.988301277160645, Val Accuracy 0.8116169544740973, Precision 0.8801198801198801, Recall 0.37851949175795185, F1 0.37849740932642484, Cohen Kappa 0.4984326018808778, Accuracy 0.8116169544740973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 155.92475287616253, Val Loss 39.362528309226036, Val Accuracy 0.8194662480376766, Precision 0.8908426205095028, Recall 0.3746359146457291, F1 0.37948297859186275, Cohen Kappa 0.5009715913832362, Accuracy 0.8194662480376766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 141.0815872400999, Val Loss 37.60077890753746, Val Accuracy 0.8163265306122449, Precision 0.8813211082853701, Recall 0.39163975601004664, F1 0.38639793909207537, Cohen Kappa 0.52878955521133, Accuracy 0.8163265306122449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 134.79204474389553, Val Loss 37.631539180874825, Val Accuracy 0.8139717425431711, Precision 0.87836929221073, Recall 0.398502852881375, F1 0.3878329935397889, Cohen Kappa 0.538649125621691, Accuracy 0.8139717425431711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 126.6433861553669, Val Loss 35.99930666387081, Val Accuracy 0.8398744113029827, Precision 0.9040551863457411, Recall 0.39423408401753235, F1 0.39736837781741113, Cohen Kappa 0.5665806707680096, Accuracy 0.8398744113029827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 117.8436641804874, Val Loss 37.954836927354336, Val Accuracy 0.8320251177394035, Precision 0.9029901715808762, Recall 0.3816810541941929, F1 0.38820343291146897, Cohen Kappa 0.5322123468652413, Accuracy 0.8320251177394035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 112.03538370877504, Val Loss 37.32979540526867, Val Accuracy 0.8163265306122449, Precision 0.8814635391569721, Recall 0.4151988926176857, F1 0.3950937263913372, Cohen Kappa 0.5663666354416975, Accuracy 0.8163265306122449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 106.26485687121749, Val Loss 40.30008526146412, Val Accuracy 0.8202511773940345, Precision 0.9037798601670227, Recall 0.36339939635703583, F1 0.37351469480633936, Cohen Kappa 0.4770639899623589, Accuracy 0.8202511773940345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 101.06776409223676, Val Loss 39.16653189063072, Val Accuracy 0.8296703296703297, Precision 0.8883125430953426, Recall 0.41648638285597694, F1 0.40069856095153933, Cohen Kappa 0.5877840286403155, Accuracy 0.8296703296703297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 92.71020561270416, Val Loss 36.16768365353346, Val Accuracy 0.8547880690737834, Precision 0.9080814832820181, Recall 0.4174273764044549, F1 0.41265835977687504, Cohen Kappa 0.6259886252554875, Accuracy 0.8547880690737834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 87.04786639288068, Val Loss 41.30411997437477, Val Accuracy 0.8037676609105181, Precision 0.5376075104970879, Recall 0.4543589233325192, F1 0.44530444285999726, Cohen Kappa 0.5591841931894304, Accuracy 0.8037676609105181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 79.35598063841462, Val Loss 40.6194786131382, Val Accuracy 0.8437990580847724, Precision 0.41464495553562597, Recall 0.3901007478700127, F1 0.8979900140827038, Cohen Kappa 0.5665659128365395, Accuracy 0.8437990580847724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['av'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_AV_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_AV_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSWwXokYpcL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-AV')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcpgXjubYp4g",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 91.4372282139957, Val Loss 24.64504837244749, Val Accuracy 0.9089481946624803, Precision 0.9544740973312402, Recall 0.5, F1 0.4761513157894737, Cohen Kappa 0.0, Accuracy 0.9089481946624803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 79.85258263722062, Val Loss 22.130836058408022, Val Accuracy 0.9089481946624803, Precision 0.9544740973312402, Recall 0.5, F1 0.4761513157894737, Cohen Kappa 0.0, Accuracy 0.9089481946624803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 73.28181539662182, Val Loss 21.16892613656819, Val Accuracy 0.9089481946624803, Precision 0.9544740973312402, Recall 0.5, F1 0.4761513157894737, Cohen Kappa 0.0, Accuracy 0.9089481946624803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 67.78696643933654, Val Loss 20.929510310292244, Val Accuracy 0.9105180533751962, Precision 0.9551886792452831, Recall 0.5086206896551724, F1 0.4934923624189161, Cohen Kappa 0.03090719833453437, Accuracy 0.9105180533751962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 63.01593896653503, Val Loss 21.232246699742973, Val Accuracy 0.9144427001569859, Precision 0.9017566974088713, Recall 0.5340509796915014, F1 0.5415072224515064, Cohen Kappa 0.11641490945648447, Accuracy 0.9144427001569859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 58.02383022662252, Val Loss 22.311106923036277, Val Accuracy 0.9144427001569859, Precision 0.9569850039463299, Recall 0.5301724137931034, F1 0.5344363423015673, Cohen Kappa 0.10454094068791187, Accuracy 0.9144427001569859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 54.855760876089334, Val Loss 18.72271779179573, Val Accuracy 0.9160125588697017, Precision 0.7620125197246355, Recall 0.612485855517837, F1 0.6493472752762023, Cohen Kappa 0.3071793777127232, Accuracy 0.9160125588697017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 49.44638045039028, Val Loss 18.547888856381178, Val Accuracy 0.923861852433281, Precision 0.8497982243744956, Recall 0.6129250789113215, F1 0.6585743216548279, Cohen Kappa 0.32930628371704285, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 45.12742840964347, Val Loss 23.27641502674669, Val Accuracy 0.9191522762951334, Precision 0.9010014506996116, Recall 0.5637916145554166, F1 0.5914569031273836, Cohen Kappa 0.20710824299991548, Accuracy 0.9191522762951334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 40.49960999703035, Val Loss 19.8054351657629, Val Accuracy 0.923861852433281, Precision 0.8428084511350478, Recall 0.6168036448097195, F1 0.6627560140270443, Cohen Kappa 0.3368073071515204, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 35.390217014588416, Val Loss 20.65218025725335, Val Accuracy 0.9222919937205651, Precision 0.8028571428571428, Recall 0.6353329164433328, F1 0.6792278640369283, Cohen Kappa 0.36569738787580086, Accuracy 0.9222919937205651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 30.592737526400015, Val Loss 26.866656347177923, Val Accuracy 0.9215070643642073, Precision 0.8657852564102564, Recall 0.5883583467333691, F1 0.6271059440600377, Cohen Kappa 0.27148379423134106, Accuracy 0.9215070643642073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 28.35905955522321, Val Loss 25.834133349824697, Val Accuracy 0.923861852433281, Precision 0.8213814302504294, Recall 0.6323179084033113, F1 0.6784393726890221, Cohen Kappa 0.36520542034375414, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 21.771100176032633, Val Loss 23.198393911123276, Val Accuracy 0.9120879120879121, Precision 0.7317457520945894, Recall 0.6762625811446609, F1 0.6989018594954124, Cohen Kappa 0.3989452223289356, Accuracy 0.9120879120879121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 17.38968022901099, Val Loss 26.52277078991756, Val Accuracy 0.9199372056514914, Precision 0.77037606087211, Recall 0.6650661068429515, F1 0.7012524828956079, Cohen Kappa 0.40565312843029633, Accuracy 0.9199372056514914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 14.803752935375087, Val Loss 28.9772724295035, Val Accuracy 0.9128728414442701, Precision 0.735186681184363, Recall 0.6495443987850634, F1 0.6797015675855649, Cohen Kappa 0.3623278590947214, Accuracy 0.9128728414442701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 15.214181439892855, Val Loss 30.679285348625854, Val Accuracy 0.9262166405023547, Precision 0.8412601626016261, Recall 0.637491811089274, F1 0.6865682579564489, Cohen Kappa 0.3815278466369195, Accuracy 0.9262166405023547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 12.709418647893472, Val Loss 31.01791938045062, Val Accuracy 0.9191522762951334, Precision 0.7721629350631234, Recall 0.6452414984217736, F1 0.6840117994100295, Cohen Kappa 0.37292962888627657, Accuracy 0.9191522762951334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 9.180428760795621, Val Loss 30.197779747541063, Val Accuracy 0.9065934065934066, Precision 0.7124190884335521, Recall 0.6771186945387411, F1 0.6925494872309661, Cohen Kappa 0.3856484070445023, Accuracy 0.9065934065934066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 9.926575711375335, Val Loss 33.6582699684659, Val Accuracy 0.923861852433281, Precision 0.8309793681878425, Recall 0.6245607766065154, F1 0.6707984027150647, Cohen Kappa 0.3513170188865442, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 7.227225346956402, Val Loss 34.03140655712923, Val Accuracy 0.9152276295133438, Precision 0.7465946843853821, Recall 0.6547183014710262, F1 0.6868154379831197, Cohen Kappa 0.37663326144868703, Accuracy 0.9152276295133438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 7.28701379217091, Val Loss 31.99923851771746, Val Accuracy 0.9097331240188383, Precision 0.724115127574035, Recall 0.6904815079506879, F1 0.7054216806908546, Cohen Kappa 0.4112706844867354, Accuracy 0.9097331240188383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 7.46373056867742, Val Loss 36.56572263533599, Val Accuracy 0.923861852433281, Precision 0.813469387755102, Recall 0.6400750402001072, F1 0.6857081092078994, Cohen Kappa 0.37851158206012814, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 5.54259507257666, Val Loss 37.16426074432093, Val Accuracy 0.9136577708006279, Precision 0.7382943143812709, Recall 0.6654904413078435, F1 0.6931303593795164, Cohen Kappa 0.3881952326901248, Accuracy 0.9136577708006279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 7.425036231892591, Val Loss 36.878537950105965, Val Accuracy 0.9183673469387755, Precision 0.7581867671691792, Recall 0.6835953784765648, F1 0.7125850340136055, Cohen Kappa 0.42678157339147893, Accuracy 0.9183673469387755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 5.113396953383926, Val Loss 38.57538056417252, Val Accuracy 0.9065934065934066, Precision 0.7124190884335521, Recall 0.6771186945387411, F1 0.6925494872309661, Cohen Kappa 0.3856484070445023, Accuracy 0.9065934065934066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 5.666579512464523, Val Loss 39.471864989813184, Val Accuracy 0.9058084772370487, Precision 0.7080611861861862, Recall 0.6650512179143588, F1 0.6831187351280542, Cohen Kappa 0.36712424036694213, Accuracy 0.9058084772370487\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 4.864799626440799, Val Loss 36.108492729093996, Val Accuracy 0.9191522762951334, Precision 0.76501820565956, Recall 0.6685128938121613, F1 0.7027861392911097, Cohen Kappa 0.4082862115923992, Accuracy 0.9191522762951334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 4.115691382181467, Val Loss 41.39785573774134, Val Accuracy 0.902668759811617, Precision 0.7009552967944856, Recall 0.6788383657911976, F1 0.6889868882151435, Cohen Kappa 0.3782137065667459, Accuracy 0.902668759811617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 3.2209680784944794, Val Loss 39.13795166925411, Val Accuracy 0.9065934065934066, Precision 0.7124190884335521, Recall 0.6771186945387411, F1 0.6925494872309661, Cohen Kappa 0.3856484070445023, Accuracy 0.9065934065934066\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['ac'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_AC_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_AC_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-AC')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.rnn.flatten_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErMZHbTKYqKl",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 273.9455453157425, Val Loss 61.798553586006165, Val Accuracy 0.6679748822605965, Precision 0.8893249607535322, Recall 0.3333333333333333, F1 0.2669803921568627, Cohen Kappa 0.0, Accuracy 0.6679748822605965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 247.380857527256, Val Loss 55.32634714245796, Val Accuracy 0.7111459968602826, Precision 0.7656418147622928, Recall 0.4364538451494973, F1 0.42935243180370475, Cohen Kappa 0.3017868152750516, Accuracy 0.7111459968602826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 231.27080675959587, Val Loss 53.24464997649193, Val Accuracy 0.7221350078492935, Precision 0.7751313868613138, Recall 0.44825695260477866, F1 0.4410289799543654, Cohen Kappa 0.33436844140471644, Accuracy 0.7221350078492935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 217.47683024406433, Val Loss 50.95131394267082, Val Accuracy 0.7448979591836735, Precision 0.794973544973545, Recall 0.49408538973756366, F1 0.47726630358623084, Cohen Kappa 0.4329402741827245, Accuracy 0.7448979591836735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 206.7768759727478, Val Loss 51.13759684562683, Val Accuracy 0.7339089481946625, Precision 0.7874833827589733, Recall 0.4874526700613657, F1 0.47008869994226626, Cohen Kappa 0.4113684933597924, Accuracy 0.7339089481946625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 196.84789894521236, Val Loss 49.29054528474808, Val Accuracy 0.7621664050235479, Precision 0.5627470128065618, Recall 0.48934988500205895, F1 0.4872458458789411, Cohen Kappa 0.4382814137039883, Accuracy 0.7621664050235479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 186.67294700443745, Val Loss 48.02900870144367, Val Accuracy 0.7708006279434851, Precision 0.5751244386322395, Recall 0.49409342452820715, F1 0.4984919560351861, Cohen Kappa 0.451038724548371, Accuracy 0.7708006279434851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 178.65114556252956, Val Loss 50.072008803486824, Val Accuracy 0.7394034536891679, Precision 0.5461764803042959, Recall 0.5088734219169001, F1 0.4953624530815739, Cohen Kappa 0.4398338710275522, Accuracy 0.7394034536891679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 169.6228485107422, Val Loss 48.280234172940254, Val Accuracy 0.7708006279434851, Precision 0.5641595441595442, Recall 0.5152098586881195, F1 0.5159541402673811, Cohen Kappa 0.4785429232647558, Accuracy 0.7708006279434851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 159.67985251545906, Val Loss 48.54339152574539, Val Accuracy 0.7645211930926217, Precision 0.5993649951264625, Recall 0.5446985446985447, F1 0.5492395256586814, Cohen Kappa 0.4896930547785533, Accuracy 0.7645211930926217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 149.16612766683102, Val Loss 49.25443236529827, Val Accuracy 0.7810047095761381, Precision 0.6065105558174865, Recall 0.5389807868068738, F1 0.5420228040441547, Cohen Kappa 0.5114795690471511, Accuracy 0.7810047095761381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 141.34141243249178, Val Loss 49.70004164427519, Val Accuracy 0.7770800627943485, Precision 0.6378494405570766, Recall 0.5424879729227555, F1 0.5515947982164354, Cohen Kappa 0.5018682556247298, Accuracy 0.7770800627943485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 135.94061436876655, Val Loss 49.31916889548302, Val Accuracy 0.7747252747252747, Precision 0.5935024826019482, Recall 0.5453503670894975, F1 0.5600558647910691, Cohen Kappa 0.4934786614043424, Accuracy 0.7747252747252747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 121.36329797655344, Val Loss 52.75370669364929, Val Accuracy 0.7786499215070644, Precision 0.6009232728430436, Recall 0.5471531732401297, F1 0.5551563508926695, Cohen Kappa 0.5110809299135435, Accuracy 0.7786499215070644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 113.42007283121347, Val Loss 53.379490442574024, Val Accuracy 0.7676609105180534, Precision 0.6157253406180215, Recall 0.5873813612944048, F1 0.5952380952380952, Cohen Kappa 0.5101298512348598, Accuracy 0.7676609105180534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 104.91484868526459, Val Loss 54.79354730993509, Val Accuracy 0.7802197802197802, Precision 0.6417706692515853, Recall 0.5825936304197173, F1 0.6032570915287451, Cohen Kappa 0.5123838268683729, Accuracy 0.7802197802197802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 100.53805022034794, Val Loss 55.89178602397442, Val Accuracy 0.7731554160125589, Precision 0.6110751117040425, Recall 0.5786334829813091, F1 0.5911355901815817, Cohen Kappa 0.5070154555594089, Accuracy 0.7731554160125589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 90.30744999274611, Val Loss 59.762357361614704, Val Accuracy 0.771585557299843, Precision 0.6024002384458939, Recall 0.5735333996203561, F1 0.5832782440291613, Cohen Kappa 0.5088906875297228, Accuracy 0.771585557299843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 84.51831643097103, Val Loss 60.71357046067715, Val Accuracy 0.7394034536891679, Precision 0.5817635331537537, Recall 0.5989173119607902, F1 0.5762280936358768, Cohen Kappa 0.44870553304302385, Accuracy 0.7394034536891679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 77.71088346280158, Val Loss 70.46464934200048, Val Accuracy 0.7386185243328101, Precision 0.5977703229009448, Recall 0.5754657667701145, F1 0.574010874101897, Cohen Kappa 0.4709308923571922, Accuracy 0.7386185243328101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 68.77128774253651, Val Loss 69.00822806358337, Val Accuracy 0.7441130298273155, Precision 0.5991272705025371, Recall 0.6092781744955659, F1 0.603145653152417, Cohen Kappa 0.47924889787197766, Accuracy 0.7441130298273155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 64.20689215231687, Val Loss 68.57915090024471, Val Accuracy 0.7394034536891679, Precision 0.5798988994504906, Recall 0.5765876244137114, F1 0.5751908433412632, Cohen Kappa 0.46628845569038857, Accuracy 0.7394034536891679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 55.62289715092629, Val Loss 71.76795412227511, Val Accuracy 0.7708006279434851, Precision 0.6278595132803414, Recall 0.582489178141352, F1 0.5983747833257603, Cohen Kappa 0.5015041654327373, Accuracy 0.7708006279434851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 52.15852525923401, Val Loss 77.13303883746266, Val Accuracy 0.7660910518053375, Precision 0.616979725154651, Recall 0.5715377584942802, F1 0.5873374389721228, Cohen Kappa 0.47066245968815745, Accuracy 0.7660910518053375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 56.736301676835865, Val Loss 69.44992130994797, Val Accuracy 0.7598116169544741, Precision 0.6095732946404743, Recall 0.6143792622053491, F1 0.6115397364279518, Cohen Kappa 0.5045787615612776, Accuracy 0.7598116169544741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 47.368749456480145, Val Loss 71.5752441585064, Val Accuracy 0.7637362637362637, Precision 0.6274656215341313, Recall 0.6035433426737775, F1 0.6127665516751779, Cohen Kappa 0.5003036170828262, Accuracy 0.7637362637362637\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 42.55003285780549, Val Loss 82.59014121815562, Val Accuracy 0.707221350078493, Precision 0.5711253379449924, Recall 0.6072493898580854, F1 0.5815872775016966, Cohen Kappa 0.4392155659508985, Accuracy 0.707221350078493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 41.909883056301624, Val Loss 79.55812436714768, Val Accuracy 0.7464678178963893, Precision 0.581158113908079, Recall 0.6091646830777265, F1 0.5917120978724354, Cohen Kappa 0.48839476088943035, Accuracy 0.7464678178963893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 33.690719256410375, Val Loss 82.22060792148113, Val Accuracy 0.7354788069073783, Precision 0.5750618790329843, Recall 0.6043950304819871, F1 0.5866328756926712, Cohen Kappa 0.4729345848545635, Accuracy 0.7354788069073783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 39.6245860287454, Val Loss 78.98527174256742, Val Accuracy 0.7472527472527473, Precision 0.591088900621206, Recall 0.594480098827925, F1 0.590324416926487, Cohen Kappa 0.4610071462263221, Accuracy 0.7472527472527473\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['pr'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_PR_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_PR_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-PR')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYHoWE1eYqbD",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 196.44880506396294, Val Loss 41.622448056936264, Val Accuracy 0.7378335949764521, Precision 0.7220552966505873, Recall 0.6757901833872708, F1 0.684941122115339, Cohen Kappa 0.3799384479080449, Accuracy 0.7378335949764521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 154.01203700900078, Val Loss 37.010655134916306, Val Accuracy 0.783359497645212, Precision 0.7703850441019676, Recall 0.7407389428263215, F1 0.7505561735261401, Cohen Kappa 0.5038380782516425, Accuracy 0.783359497645212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 139.2878744006157, Val Loss 34.04721845686436, Val Accuracy 0.8021978021978022, Precision 0.7895804073033708, Recall 0.7669012944983818, F1 0.7754086112673455, Cohen Kappa 0.5521818455718661, Accuracy 0.8021978021978022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 129.28444010019302, Val Loss 37.471065670251846, Val Accuracy 0.7864992150706437, Precision 0.7702137546468402, Recall 0.7885544768069039, F1 0.7751686909581647, Cohen Kappa 0.5525853698845209, Accuracy 0.7864992150706437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 117.4091335684061, Val Loss 32.126661613583565, Val Accuracy 0.8210361067503925, Precision 0.8101811259806184, Recall 0.7900377562028047, F1 0.7980275015019024, Cohen Kappa 0.5969339046646389, Accuracy 0.8210361067503925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 107.4572621434927, Val Loss 32.26643854379654, Val Accuracy 0.8178963893249608, Precision 0.8269568721756824, Recall 0.7669336569579288, F1 0.7834685702396755, Cohen Kappa 0.5725598856951137, Accuracy 0.8178963893249608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 99.81356232613325, Val Loss 31.76809598505497, Val Accuracy 0.8304552590266876, Precision 0.8167952275249722, Recall 0.8079099244875945, F1 0.8119140769173987, Cohen Kappa 0.6240018800999076, Accuracy 0.8304552590266876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 91.62662468478084, Val Loss 31.92821356654167, Val Accuracy 0.8437990580847724, Precision 0.83786472630096, Recall 0.8136866235167206, F1 0.8231852542558467, Cohen Kappa 0.6472858402918253, Accuracy 0.8437990580847724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 84.46190058998764, Val Loss 35.383440122008324, Val Accuracy 0.8335949764521193, Precision 0.81660477037347, Recall 0.826475188781014, F1 0.8208342952060265, Cohen Kappa 0.6419431644032474, Accuracy 0.8335949764521193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 79.64265508949757, Val Loss 32.51240487769246, Val Accuracy 0.8469387755102041, Precision 0.8500545256270446, Recall 0.8090533980582524, F1 0.8231798586036739, Cohen Kappa 0.6485233822568186, Accuracy 0.8469387755102041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 74.12213508412242, Val Loss 32.55968535318971, Val Accuracy 0.8430141287284144, Precision 0.8312028569521394, Recall 0.8211488673139159, F1 0.8256552297957128, Cohen Kappa 0.6514974122178332, Accuracy 0.8430141287284144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 69.83187694847584, Val Loss 32.6801168769598, Val Accuracy 0.8437990580847724, Precision 0.8343960506590264, Recall 0.818225458468177, F1 0.8250563936883233, Cohen Kappa 0.6505586353474345, Accuracy 0.8437990580847724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 62.12527628801763, Val Loss 35.198629355058074, Val Accuracy 0.8445839874411303, Precision 0.8310365733761552, Recall 0.8269012944983819, F1 0.8288749592966461, Cohen Kappa 0.6577833490704262, Accuracy 0.8445839874411303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 59.013804676011205, Val Loss 32.59776384755969, Val Accuracy 0.8461538461538461, Precision 0.8399943310657596, Recall 0.8170199568500539, F1 0.8261661371211847, Cohen Kappa 0.6531445684726382, Accuracy 0.8461538461538461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 50.750147317536175, Val Loss 36.062493711709976, Val Accuracy 0.8508634222919937, Precision 0.8539691222951618, Recall 0.8141046386192017, F1 0.8280555200386424, Cohen Kappa 0.6580783682232958, Accuracy 0.8508634222919937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 47.36594922840595, Val Loss 37.87876382563263, Val Accuracy 0.8618524332810047, Precision 0.857490048549274, Recall 0.835207659115426, F1 0.8442772096796147, Cohen Kappa 0.6891838692327739, Accuracy 0.8618524332810047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 41.96180258644745, Val Loss 52.649028051644564, Val Accuracy 0.8390894819466248, Precision 0.8646219961470729, Recall 0.785334412081985, F1 0.8058716450105439, Cohen Kappa 0.6183834079996727, Accuracy 0.8390894819466248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 45.59679571026936, Val Loss 43.79907730035484, Val Accuracy 0.8461538461538461, Precision 0.8300246641477749, Recall 0.8392098166127293, F1 0.8340550527001342, Cohen Kappa 0.6683084933981562, Accuracy 0.8461538461538461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 34.19632046739571, Val Loss 48.350636325310916, Val Accuracy 0.8445839874411303, Precision 0.8281812818283443, Recall 0.8420307443365695, F1 0.8336946202531645, Cohen Kappa 0.6678937057633973, Accuracy 0.8445839874411303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 35.846552283037454, Val Loss 43.07177084637806, Val Accuracy 0.8500784929356358, Precision 0.8377069528467429, Recall 0.831653182308522, F1 0.8344886222575916, Cohen Kappa 0.6690423017183458, Accuracy 0.8500784929356358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 30.502478846348822, Val Loss 46.278401772491634, Val Accuracy 0.8555729984301413, Precision 0.8501616391707507, Recall 0.8283360302049623, F1 0.8371989010286882, Cohen Kappa 0.6750558632888091, Accuracy 0.8555729984301413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 30.5362596756313, Val Loss 42.69914714014158, Val Accuracy 0.858712715855573, Precision 0.8483091947913899, Recall 0.839336569579288, F1 0.8434325227764305, Cohen Kappa 0.6869881412566337, Accuracy 0.858712715855573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 24.208025830448605, Val Loss 49.78283608634956, Val Accuracy 0.8634222919937206, Precision 0.8607466063348417, Recall 0.8354126213592232, F1 0.8454918032786886, Cohen Kappa 0.6917570498915402, Accuracy 0.8634222919937206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 28.801361149409786, Val Loss 52.314977454487234, Val Accuracy 0.8540031397174255, Precision 0.8565934065934067, Recall 0.8185490830636462, F1 0.8321161677563063, Cohen Kappa 0.6659825551564905, Accuracy 0.8540031397174255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 22.160467711277306, Val Loss 50.98872543917969, Val Accuracy 0.8563579277864992, Precision 0.8526672766618739, Recall 0.8274298813376484, F1 0.8374015152202006, Cohen Kappa 0.6756447677055479, Accuracy 0.8563579277864992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 22.921909848810174, Val Loss 47.640285819768906, Val Accuracy 0.858712715855573, Precision 0.8513534166553458, Recall 0.8347977346278317, F1 0.8418504572350727, Cohen Kappa 0.6840852358202048, Accuracy 0.858712715855573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 16.823418326093815, Val Loss 54.31338835740462, Val Accuracy 0.859497645211931, Precision 0.8583926309323316, Recall 0.8288484358144552, F1 0.8401726348043596, Cohen Kappa 0.6814083381531557, Accuracy 0.859497645211931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 18.42167246917961, Val Loss 65.16158760059625, Val Accuracy 0.8453689167974883, Precision 0.8581222559945965, Recall 0.8002750809061489, F1 0.8178746650160262, Cohen Kappa 0.6395786002935322, Accuracy 0.8453689167974883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 26.09798355353996, Val Loss 49.315867621218786, Val Accuracy 0.8571428571428571, Precision 0.8434855629481299, Recall 0.8441747572815534, F1 0.8438275413351726, Cohen Kappa 0.6876559241757235, Accuracy 0.8571428571428571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 16.583879843121395, Val Loss 54.57069442351349, Val Accuracy 0.859497645211931, Precision 0.850291001967927, Recall 0.8384304207119742, F1 0.843702516224587, Cohen Kappa 0.6876107867606568, Accuracy 0.859497645211931\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['ui'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_UI_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_UI_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-UI')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCr4-G0lYqof",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 145.70127841830254, Val Loss 30.50500827282667, Val Accuracy 0.8524332810047096, Precision 0.8294463236603169, Recall 0.5736947604571823, F1 0.5889447995112207, Cohen Kappa 0.21761343220200569, Accuracy 0.8524332810047096\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 105.91509138047695, Val Loss 26.422154635190964, Val Accuracy 0.880690737833595, Precision 0.8705387621827536, Recall 0.6657064599345998, F1 0.7111886651752424, Cohen Kappa 0.4361551586585216, Accuracy 0.880690737833595\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 92.03275469690561, Val Loss 23.09811001084745, Val Accuracy 0.9003139717425431, Precision 0.849557885355099, Recall 0.7637913563694451, F1 0.7969389868159313, Cohen Kappa 0.5957657311034382, Accuracy 0.9003139717425431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 79.56611688435078, Val Loss 22.85699190199375, Val Accuracy 0.8979591836734694, Precision 0.9002180527803803, Recall 0.7135973238109146, F1 0.7651819336690713, Cohen Kappa 0.5382848349344589, Accuracy 0.8979591836734694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 76.43277411907911, Val Loss 21.2952564638108, Val Accuracy 0.9081632653061225, Precision 0.8397392923649907, Recall 0.8229126565867084, F1 0.8309530425118088, Cohen Kappa 0.6619708727736177, Accuracy 0.9081632653061225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 66.72651432827115, Val Loss 19.484459412284195, Val Accuracy 0.923861852433281, Precision 0.9055844825968664, Recall 0.8060714269911015, F1 0.8449061552846089, Cohen Kappa 0.6912541410790041, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 61.366328962147236, Val Loss 18.41175521351397, Val Accuracy 0.9262166405023547, Precision 0.8915384332687573, Recall 0.829999159265995, F1 0.8564992115341292, Cohen Kappa 0.7135476216673922, Accuracy 0.9262166405023547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 54.97617259901017, Val Loss 18.796532666310668, Val Accuracy 0.9262166405023547, Precision 0.9217908528253356, Recall 0.8018566946763838, F1 0.8465359222504716, Cohen Kappa 0.6949870869478842, Accuracy 0.9262166405023547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 50.06638289056718, Val Loss 22.182807869277894, Val Accuracy 0.9230769230769231, Precision 0.9139136725343622, Recall 0.7962193519268296, F1 0.8400055359632574, Cohen Kappa 0.6820078140520496, Accuracy 0.9230769230769231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 54.40636007860303, Val Loss 17.121594260446727, Val Accuracy 0.9293563579277865, Precision 0.8982270453030088, Recall 0.8356365020155492, F1 0.862605628064592, Cohen Kappa 0.7257370845751627, Accuracy 0.9293563579277865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 49.117362015880644, Val Loss 18.342879860661924, Val Accuracy 0.9301412872841445, Precision 0.9024616368286444, Recall 0.8342315912439766, F1 0.8632577379265431, Cohen Kappa 0.727123954928981, Accuracy 0.9301412872841445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 37.809944360051304, Val Loss 17.862914571538568, Val Accuracy 0.9309262166405023, Precision 0.8821734755857247, Recall 0.8647214736739635, F1 0.8731012500622541, Cohen Kappa 0.746243860483013, Accuracy 0.9309262166405023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 38.846966966055334, Val Loss 18.859555506147444, Val Accuracy 0.9285714285714286, Precision 0.8794121512522999, Recall 0.8576792201528366, F1 0.8680028737104453, Cohen Kappa 0.7360733587393515, Accuracy 0.9285714285714286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 34.12985838903114, Val Loss 20.363004353363067, Val Accuracy 0.9340659340659341, Precision 0.9197624907222939, Recall 0.832835530304036, F1 0.8683813075284477, Cohen Kappa 0.7376351467307363, Accuracy 0.9340659340659341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 30.02154764626175, Val Loss 19.483831780031323, Val Accuracy 0.9254317111459969, Precision 0.8922794117647059, Recall 0.8257755771196453, F1 0.8540391584609168, Cohen Kappa 0.7087278170590245, Accuracy 0.9254317111459969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 27.138475429965183, Val Loss 20.698904019314796, Val Accuracy 0.9356357927786499, Precision 0.9212172751017433, Recall 0.8375303659847871, F1 0.8720808353904628, Cohen Kappa 0.7449424540877863, Accuracy 0.9356357927786499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 40.775979498866946, Val Loss 19.74775932682678, Val Accuracy 0.9309262166405023, Precision 0.8895374637310121, Recall 0.8534644878381188, F1 0.8700440017990199, Cohen Kappa 0.7402614251889332, Accuracy 0.9309262166405023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 24.146754862973467, Val Loss 19.72862862609327, Val Accuracy 0.9356357927786499, Precision 0.8910341118267007, Recall 0.8731774877982947, F1 0.8817534375580094, Cohen Kappa 0.7635454154500804, Accuracy 0.9356357927786499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 22.349463725753594, Val Loss 20.19134692987427, Val Accuracy 0.9324960753532182, Precision 0.89545082272355, Recall 0.8525308306009478, F1 0.8719597989949749, Cohen Kappa 0.7441540063795704, Accuracy 0.9324960753532182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 20.67679121531546, Val Loss 22.095005849725567, Val Accuracy 0.9403453689167975, Precision 0.9145238660890751, Recall 0.8628718588628852, F1 0.8859117024636021, Cohen Kappa 0.7720983876662352, Accuracy 0.9403453689167975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 26.289970346027985, Val Loss 23.10567277413793, Val Accuracy 0.9332810047095761, Precision 0.8908743795341734, Recall 0.8623829056652197, F1 0.8757268584452144, Cohen Kappa 0.7515566425159564, Accuracy 0.9332810047095761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 16.568715639587026, Val Loss 22.46709914004896, Val Accuracy 0.9379905808477237, Precision 0.9105169612025188, Recall 0.8577057696477324, F1 0.8811616277902741, Cohen Kappa 0.7626284781675559, Accuracy 0.9379905808477237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 14.05660187988542, Val Loss 23.964432014501654, Val Accuracy 0.9372056514913658, Precision 0.8978066110596231, Recall 0.8703676662551495, F1 0.8832691955286788, Cohen Kappa 0.7666249925581949, Accuracy 0.9372056514913658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 10.93608624933404, Val Loss 25.988269174296875, Val Accuracy 0.9387755102040817, Precision 0.9065622401841894, Recall 0.8656816804060303, F1 0.8843435754189946, Cohen Kappa 0.7688689997162381, Accuracy 0.9387755102040817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 16.883520441595465, Val Loss 24.31811842485331, Val Accuracy 0.9309262166405023, Precision 0.8833142456468388, Recall 0.8628453093679893, F1 0.872604632558816, Cohen Kappa 0.7452659992638271, Accuracy 0.9309262166405023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 22.147452178847743, Val Loss 23.281172164948657, Val Accuracy 0.9246467817896389, Precision 0.866477974706292, Recall 0.8609514453987512, F1 0.8636783136343978, Cohen Kappa 0.7273614894537154, Accuracy 0.9246467817896389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 10.915173796063755, Val Loss 26.45538453606423, Val Accuracy 0.923861852433281, Precision 0.8637227729808479, Recall 0.8623563561703238, F1 0.8630373223241252, Cohen Kappa 0.7260749482420014, Accuracy 0.923861852433281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 8.351095916819759, Val Loss 28.480996592901647, Val Accuracy 0.9136577708006279, Precision 0.8435926389355144, Recall 0.848725402999208, F1 0.846125129015965, Cohen Kappa 0.6922556645248563, Accuracy 0.9136577708006279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 8.272952811035793, Val Loss 28.901720528723672, Val Accuracy 0.9317111459968603, Precision 0.8842225325884543, Recall 0.8651927272083648, F1 0.8742984162267295, Cohen Kappa 0.7486450079598695, Accuracy 0.9317111459968603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 10.646909733477514, Val Loss 27.20756368484581, Val Accuracy 0.9301412872841445, Precision 0.8928571428571428, Recall 0.8454885770798211, F1 0.8666729379670404, Cohen Kappa 0.7336468576314665, Accuracy 0.9301412872841445\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['s'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_S_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_S_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-S')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tQJnjylYq2N",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 325.205539226532, Val Loss 75.94049578905106, Val Accuracy 0.5227629513343799, Precision 0.8407960199004975, Recall 0.3347222222222222, F1 0.23152442166354784, Cohen Kappa 0.002188612260093148, Accuracy 0.5227629513343799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 279.6162371635437, Val Loss 64.92087924480438, Val Accuracy 0.6397174254317112, Precision 0.6587769144156022, Recall 0.5539567923874728, F1 0.5693294190447379, Cohen Kappa 0.34466709775315774, Accuracy 0.6397174254317112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 244.54606395959854, Val Loss 59.49661034345627, Val Accuracy 0.6758241758241759, Precision 0.6972732087757555, Recall 0.6591544919208591, F1 0.6635133305483722, Cohen Kappa 0.4737911674138998, Accuracy 0.6758241758241759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 224.07820531725883, Val Loss 56.09073632955551, Val Accuracy 0.7001569858712716, Precision 0.7086016204727504, Recall 0.6780239046124796, F1 0.6865724304019233, Cohen Kappa 0.5059077701543391, Accuracy 0.7001569858712716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 203.57213287055492, Val Loss 53.66365538537502, Val Accuracy 0.7276295133437991, Precision 0.7168135623834077, Recall 0.6957650732522364, F1 0.7050135023127724, Cohen Kappa 0.5448969507298895, Accuracy 0.7276295133437991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 190.94586154818535, Val Loss 53.46051728725433, Val Accuracy 0.7087912087912088, Precision 0.7013245461006655, Recall 0.7032856531572834, F1 0.6976439970945315, Cohen Kappa 0.5311855900177546, Accuracy 0.7087912087912088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 177.38046741485596, Val Loss 55.11785668134689, Val Accuracy 0.7119309262166404, Precision 0.6947020435105199, Recall 0.7106316774320626, F1 0.7000255062419161, Cohen Kappa 0.5378021089425928, Accuracy 0.7119309262166404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 165.92927791178226, Val Loss 50.020857110619545, Val Accuracy 0.7511773940345369, Precision 0.7488962602981121, Recall 0.7217425338413785, F1 0.7328252774756002, Cohen Kappa 0.5846650493075145, Accuracy 0.7511773940345369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 154.2913547307253, Val Loss 60.034610986709595, Val Accuracy 0.7111459968602826, Precision 0.6887188688858498, Recall 0.7274810501864417, F1 0.7000930995852159, Cohen Kappa 0.545974234246972, Accuracy 0.7111459968602826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 151.26877062022686, Val Loss 51.87110535800457, Val Accuracy 0.7590266875981162, Precision 0.7693916226869705, Recall 0.7170214662672943, F1 0.7373499113516088, Cohen Kappa 0.5894907757752977, Accuracy 0.7590266875981162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 138.93145985156298, Val Loss 56.64656452834606, Val Accuracy 0.7307692307692307, Precision 0.7494915414824265, Recall 0.6820507399936154, F1 0.694029687724115, Cohen Kappa 0.5331179390528207, Accuracy 0.7307692307692307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 125.06258462741971, Val Loss 57.99618776887655, Val Accuracy 0.7543171114599686, Precision 0.7429670607943589, Recall 0.7336557858059783, F1 0.7305560786323957, Cohen Kappa 0.5919908813345667, Accuracy 0.7543171114599686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 124.57196014747024, Val Loss 57.181305937469006, Val Accuracy 0.7629513343799058, Precision 0.7672169049875546, Recall 0.7205974794710354, F1 0.7384096107346014, Cohen Kappa 0.5955287693813425, Accuracy 0.7629513343799058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 108.24406927451491, Val Loss 57.666281431913376, Val Accuracy 0.7653061224489796, Precision 0.786921885853503, Recall 0.7139682709483736, F1 0.7361427550430987, Cohen Kappa 0.6004208459296898, Accuracy 0.7653061224489796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 108.45345536246896, Val Loss 53.20706898719072, Val Accuracy 0.7692307692307693, Precision 0.7519785786655087, Recall 0.7429670891320441, F1 0.7464726669464801, Cohen Kappa 0.6167735004476276, Accuracy 0.7692307692307693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 95.46778794750571, Val Loss 63.171435207128525, Val Accuracy 0.7605965463108321, Precision 0.7888790926401823, Recall 0.712053168150729, F1 0.7280356344643274, Cohen Kappa 0.5977761077538108, Accuracy 0.7605965463108321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 88.96233869716525, Val Loss 58.46053737401962, Val Accuracy 0.7723704866562009, Precision 0.7512830641840783, Recall 0.7716238428048441, F1 0.7592211724131811, Cohen Kappa 0.6342458213382014, Accuracy 0.7723704866562009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 87.37525088898838, Val Loss 58.23884500190616, Val Accuracy 0.7810047095761381, Precision 0.7804370745823572, Recall 0.7399859744211477, F1 0.7564223088897944, Cohen Kappa 0.629569338897064, Accuracy 0.7810047095761381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 74.6636397652328, Val Loss 58.39679413288832, Val Accuracy 0.783359497645212, Precision 0.782068521647186, Recall 0.7538862399902194, F1 0.7649366130644241, Cohen Kappa 0.6388992725076919, Accuracy 0.783359497645212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 89.11455624736845, Val Loss 60.64241721481085, Val Accuracy 0.7668759811616954, Precision 0.7869419344279839, Recall 0.7130363170799628, F1 0.7370945312063016, Cohen Kappa 0.5953648184329313, Accuracy 0.7668759811616954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 68.81649691052735, Val Loss 58.81115175783634, Val Accuracy 0.7708006279434851, Precision 0.7550769358941744, Recall 0.7442047700552193, F1 0.7490777349996672, Cohen Kappa 0.619108466770488, Accuracy 0.7708006279434851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 61.90409389231354, Val Loss 64.55255936831236, Val Accuracy 0.7794348508634223, Precision 0.7918578970815443, Recall 0.7413656091448133, F1 0.7572477650063858, Cohen Kappa 0.630375773720103, Accuracy 0.7794348508634223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 60.56392113538459, Val Loss 67.16427113115788, Val Accuracy 0.7849293563579278, Precision 0.7867254490620805, Recall 0.7501471327369916, F1 0.76509497608291, Cohen Kappa 0.6383902442561629, Accuracy 0.7849293563579278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 59.01484732050449, Val Loss 61.8344447389245, Val Accuracy 0.7786499215070644, Precision 0.7695674829071413, Recall 0.7511442902649579, F1 0.7594941980858033, Cohen Kappa 0.6310519089347202, Accuracy 0.7786499215070644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 48.31021782755852, Val Loss 75.2488357424736, Val Accuracy 0.7778649921507065, Precision 0.7928441340111276, Recall 0.7245981654678703, F1 0.7489476993235851, Cohen Kappa 0.6186911578381831, Accuracy 0.7778649921507065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 50.65602449141443, Val Loss 74.57301366329193, Val Accuracy 0.7590266875981162, Precision 0.7415464071378035, Recall 0.7546965313011526, F1 0.744571232997212, Cohen Kappa 0.6126248189988333, Accuracy 0.7590266875981162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 39.3364919857122, Val Loss 67.44623934105039, Val Accuracy 0.7762951334379906, Precision 0.7650398809649396, Recall 0.7526220700803498, F1 0.7583924258979774, Cohen Kappa 0.6289517572903637, Accuracy 0.7762951334379906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 40.61686585703865, Val Loss 74.2351203262806, Val Accuracy 0.7653061224489796, Precision 0.7511810236969788, Recall 0.7596093893269761, F1 0.7529121264359852, Cohen Kappa 0.6207952105393618, Accuracy 0.7653061224489796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 41.2815290116705, Val Loss 73.81674285978079, Val Accuracy 0.7888540031397174, Precision 0.7841361698929442, Recall 0.7694880833520115, F1 0.7745067312842421, Cohen Kappa 0.6524138910300621, Accuracy 0.7888540031397174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 30.603754030773416, Val Loss 77.60899545624852, Val Accuracy 0.7817896389324961, Precision 0.7768883731214661, Recall 0.7520947524638153, F1 0.762779789594881, Cohen Kappa 0.6355951473322365, Accuracy 0.7817896389324961\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['i'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_I_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_I_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-I')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48UHT6y0YrLn",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 303.8349564373493, Val Loss 69.20509979128838, Val Accuracy 0.6208791208791209, Precision 0.7864625814310683, Recall 0.39771836007130124, F1 0.3622691141991193, Cohen Kappa 0.1302475932556032, Accuracy 0.6208791208791209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 257.3360586762428, Val Loss 62.76530611515045, Val Accuracy 0.6773940345368917, Precision 0.6484213083927278, Recall 0.5574628402610592, F1 0.5828836430355794, Cohen Kappa 0.37111279792074026, Accuracy 0.6773940345368917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 231.74247840046883, Val Loss 61.94598659873009, Val Accuracy 0.6750392464678179, Precision 0.662978986199129, Recall 0.5641307852369488, F1 0.5870905398694849, Cohen Kappa 0.37314699377111804, Accuracy 0.6750392464678179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 214.5860606431961, Val Loss 57.79856702685356, Val Accuracy 0.7025117739403454, Precision 0.6916230427858335, Recall 0.5827305395175093, F1 0.61315342963785, Cohen Kappa 0.4173452395317968, Accuracy 0.7025117739403454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 202.5626704096794, Val Loss 56.25856663286686, Val Accuracy 0.7197802197802198, Precision 0.6928121348937603, Recall 0.6311092173196696, F1 0.6536024935826915, Cohen Kappa 0.4751126939686511, Accuracy 0.7197802197802198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 186.28270222246647, Val Loss 60.33055354654789, Val Accuracy 0.7119309262166404, Precision 0.7564229787709279, Recall 0.5731408767780927, F1 0.612235943479558, Cohen Kappa 0.41447220121123474, Accuracy 0.7119309262166404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 176.2671024054289, Val Loss 53.293609231710434, Val Accuracy 0.7409733124018838, Precision 0.7076237858851245, Recall 0.6719386979710391, F1 0.6866047092907461, Cohen Kappa 0.5279680237127524, Accuracy 0.7409733124018838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 162.678121894598, Val Loss 54.94201549887657, Val Accuracy 0.7409733124018838, Precision 0.719408765190826, Recall 0.6696840445703814, F1 0.6894135365687921, Cohen Kappa 0.5237102610405824, Accuracy 0.7409733124018838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 156.43130185455084, Val Loss 61.00459298491478, Val Accuracy 0.7291993720565149, Precision 0.7805970149253731, Recall 0.5940944244412724, F1 0.6378464798914415, Cohen Kappa 0.4509999362978562, Accuracy 0.7291993720565149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 153.9737350270152, Val Loss 58.45643622428179, Val Accuracy 0.750392464678179, Precision 0.7387882070980663, Recall 0.666536918280532, F1 0.6862273070180261, Cohen Kappa 0.5360173713468979, Accuracy 0.750392464678179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 141.23935236781836, Val Loss 55.34906457364559, Val Accuracy 0.7370486656200942, Precision 0.7265857143415032, Recall 0.6568857546640533, F1 0.678604511255782, Cohen Kappa 0.5123698489430815, Accuracy 0.7370486656200942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 127.98646733909845, Val Loss 56.32470566779375, Val Accuracy 0.7551020408163265, Precision 0.7217111290993662, Recall 0.6973760572682531, F1 0.7082441799111012, Cohen Kappa 0.559487593798728, Accuracy 0.7551020408163265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 121.22268578782678, Val Loss 56.5272665694356, Val Accuracy 0.7527472527472527, Precision 0.7300898803975232, Recall 0.6974299593071565, F1 0.7088903089374122, Cohen Kappa 0.5554208939218366, Accuracy 0.7527472527472527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 114.95168903842568, Val Loss 67.78983671218157, Val Accuracy 0.7629513343799058, Precision 0.7904862993139176, Recall 0.6507172024912826, F1 0.6924732904557303, Cohen Kappa 0.5353251352062938, Accuracy 0.7629513343799058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 105.62263964489102, Val Loss 61.09170959889889, Val Accuracy 0.7354788069073783, Precision 0.7503571371743584, Recall 0.6379016554104439, F1 0.6667006078699417, Cohen Kappa 0.4956079600657425, Accuracy 0.7354788069073783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 96.14976434223354, Val Loss 60.510645136237144, Val Accuracy 0.7637362637362637, Precision 0.7454970972394039, Recall 0.6936148454311097, F1 0.7145675243173085, Cohen Kappa 0.5650052180225964, Accuracy 0.7637362637362637\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 87.72962335497141, Val Loss 69.35078014619648, Val Accuracy 0.7645211930926217, Precision 0.7538221476956655, Recall 0.6932939898161367, F1 0.7168423349466718, Cohen Kappa 0.5643823906539394, Accuracy 0.7645211930926217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 87.23336638882756, Val Loss 68.98774345964193, Val Accuracy 0.7448979591836735, Precision 0.7240639907393055, Recall 0.6839627585913033, F1 0.6984856106314942, Cohen Kappa 0.5372926170206584, Accuracy 0.7448979591836735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 75.48470961675048, Val Loss 66.37207127735019, Val Accuracy 0.7480376766091051, Precision 0.7358563722582012, Recall 0.676618517019267, F1 0.6977068241813685, Cohen Kappa 0.5354083640445149, Accuracy 0.7480376766091051\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 76.47783350013196, Val Loss 73.46545296162367, Val Accuracy 0.7386185243328101, Precision 0.7130393897592779, Recall 0.6871801918884177, F1 0.6970241909435692, Cohen Kappa 0.5319287437028473, Accuracy 0.7386185243328101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 64.16040561813861, Val Loss 74.68050412088633, Val Accuracy 0.7464678178963893, Precision 0.7111115201831008, Recall 0.6997201923145209, F1 0.7050241214441378, Cohen Kappa 0.5500212139635994, Accuracy 0.7464678178963893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 65.1817471254617, Val Loss 74.88101824186742, Val Accuracy 0.7558869701726845, Precision 0.7491041779330353, Recall 0.6683737065996264, F1 0.6975418022557204, Cohen Kappa 0.5396698334063345, Accuracy 0.7558869701726845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 59.70273115579039, Val Loss 75.9268657527864, Val Accuracy 0.7551020408163265, Precision 0.7376510934952494, Recall 0.679844188309152, F1 0.7017133247681947, Cohen Kappa 0.5464862749482864, Accuracy 0.7551020408163265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 52.9796261433512, Val Loss 85.35716450959444, Val Accuracy 0.7574568288854003, Precision 0.7602906460464599, Recall 0.662437664671084, F1 0.6959624848482306, Cohen Kappa 0.5367714286722889, Accuracy 0.7574568288854003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 48.069452464580536, Val Loss 83.4782100468874, Val Accuracy 0.7448979591836735, Precision 0.7390337764105418, Recall 0.6553971635738685, F1 0.6808376423963377, Cohen Kappa 0.5218722791004895, Accuracy 0.7448979591836735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 46.441495072096586, Val Loss 85.84556340053678, Val Accuracy 0.7409733124018838, Precision 0.7128744940560372, Recall 0.6916278557783128, F1 0.6983236530606973, Cohen Kappa 0.5392263667469658, Accuracy 0.7409733124018838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 47.28147939173505, Val Loss 78.47341024875641, Val Accuracy 0.7448979591836735, Precision 0.7273078417177995, Recall 0.6757183742747371, F1 0.6937561553808272, Cohen Kappa 0.533021224667487, Accuracy 0.7448979591836735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 38.73772454820573, Val Loss 80.27964655309916, Val Accuracy 0.7511773940345369, Precision 0.7281890801880343, Recall 0.6794704248957822, F1 0.6983708686763489, Cohen Kappa 0.543306694831784, Accuracy 0.7511773940345369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 36.10717347310856, Val Loss 92.32862903224304, Val Accuracy 0.7551020408163265, Precision 0.7369545885313725, Recall 0.6835344539844188, F1 0.7048706351486796, Cohen Kappa 0.5483814525172671, Accuracy 0.7551020408163265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 32.381251418963075, Val Loss 88.8106196206063, Val Accuracy 0.7456828885400314, Precision 0.7165816753436879, Recall 0.6912356989155676, F1 0.7008533139910899, Cohen Kappa 0.543339370130136, Accuracy 0.7456828885400314\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['c'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_C_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_C_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-C')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5XAfgcTYrYl",
        "outputId": "5340dfd7-841a-48e1-92cf-7612667e30aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 246.3052258491516, Val Loss 50.17441302537918, Val Accuracy 0.7323390894819466, Precision 0.8185901675485009, Recall 0.4773919892030128, F1 0.47650623755753435, Cohen Kappa 0.427250410675723, Accuracy 0.7323390894819466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 186.5025538355112, Val Loss 47.25835570693016, Val Accuracy 0.7511773940345369, Precision 0.8263305322128852, Recall 0.5029684439133258, F1 0.49783377541998225, Cohen Kappa 0.48621451216471345, Accuracy 0.7511773940345369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 172.51024062931538, Val Loss 47.115607634186745, Val Accuracy 0.7543171114599686, Precision 0.8443817733780131, Recall 0.4875960151550703, F1 0.4887450987185928, Cohen Kappa 0.46516537460768803, Accuracy 0.7543171114599686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 161.36383736133575, Val Loss 43.914277866482735, Val Accuracy 0.7810047095761381, Precision 0.8492736077481841, Recall 0.5211246982900527, F1 0.5177777536764341, Cohen Kappa 0.543231865284974, Accuracy 0.7810047095761381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 154.18842154741287, Val Loss 43.49668748676777, Val Accuracy 0.792778649921507, Precision 0.8552546685249589, Recall 0.5343308177953848, F1 0.5280474124622093, Cohen Kappa 0.5740948743632052, Accuracy 0.792778649921507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 144.56597147136927, Val Loss 51.46924016624689, Val Accuracy 0.7700156985871272, Precision 0.8581593034282843, Recall 0.4978782537837656, F1 0.49969246485795454, Cohen Kappa 0.49839486679880396, Accuracy 0.7700156985871272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 136.46734777465463, Val Loss 41.00946708023548, Val Accuracy 0.8021978021978022, Precision 0.8660468168088912, Recall 0.5342134987804279, F1 0.5320383779507055, Cohen Kappa 0.5850377417019956, Accuracy 0.8021978021978022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 131.36332815885544, Val Loss 40.24630040675402, Val Accuracy 0.8108320251177394, Precision 0.8713589925380228, Recall 0.5418365064034355, F1 0.5389651152363016, Cohen Kappa 0.604730351673788, Accuracy 0.8108320251177394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 123.59560911357403, Val Loss 41.44935480505228, Val Accuracy 0.7982731554160125, Precision 0.8599455755875702, Recall 0.5483818160983516, F1 0.5351522859107956, Cohen Kappa 0.5971425020824643, Accuracy 0.7982731554160125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 114.00279900431633, Val Loss 41.256040558218956, Val Accuracy 0.8076923076923077, Precision 0.8653119411546378, Recall 0.5508200508200508, F1 0.5406313202538925, Cohen Kappa 0.6109763666865252, Accuracy 0.8076923076923077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 109.82013731822371, Val Loss 41.572671204805374, Val Accuracy 0.8069073783359497, Precision 0.8679480876023974, Recall 0.5386270583120977, F1 0.5356606147566599, Cohen Kappa 0.5965297365632857, Accuracy 0.8069073783359497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 101.94535296782851, Val Loss 40.520627066493034, Val Accuracy 0.8053375196232339, Precision 0.8636516250335751, Recall 0.5500188256093769, F1 0.5390623160923299, Cohen Kappa 0.6075871388259608, Accuracy 0.8053375196232339\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 94.84919791668653, Val Loss 47.085026897490025, Val Accuracy 0.792778649921507, Precision 0.8577424572522613, Recall 0.5473641418523308, F1 0.53212957885796, Cohen Kappa 0.5894345961069432, Accuracy 0.792778649921507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 94.7793924305588, Val Loss 40.055754989385605, Val Accuracy 0.8092621664050236, Precision 0.8662011867960061, Recall 0.5617546289377633, F1 0.5620847699941132, Cohen Kappa 0.6140982012639766, Accuracy 0.8092621664050236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 87.32429495826364, Val Loss 42.37212496995926, Val Accuracy 0.8241758241758241, Precision 0.8774659863945579, Recall 0.5571952934157659, F1 0.5505831244956698, Cohen Kappa 0.6387544969606749, Accuracy 0.8241758241758241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 81.28699203394353, Val Loss 48.00678615830839, Val Accuracy 0.8061224489795918, Precision 0.8647866654509615, Recall 0.5440673865870717, F1 0.5376805123318161, Cohen Kappa 0.6015205832110291, Accuracy 0.8061224489795918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 73.23462096229196, Val Loss 46.64930245745927, Val Accuracy 0.82574568288854, Precision 0.8816210889381622, Recall 0.5640409855548286, F1 0.5709355717402927, Cohen Kappa 0.6372837447899967, Accuracy 0.82574568288854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 69.89134555123746, Val Loss 48.80991540104151, Val Accuracy 0.8076923076923077, Precision 0.8651640299098861, Recall 0.5596019614307651, F1 0.5607267616625778, Cohen Kappa 0.6096916222438136, Accuracy 0.8076923076923077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 64.87011504918337, Val Loss 46.45425679162145, Val Accuracy 0.8139717425431711, Precision 0.8695719529052862, Recall 0.5532973918800691, F1 0.5444060661451965, Cohen Kappa 0.6215585463861446, Accuracy 0.8139717425431711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 66.48331953911111, Val Loss 46.672021394595504, Val Accuracy 0.8116169544740973, Precision 0.8678521805905483, Recall 0.5715933201328172, F1 0.5824377460347124, Cohen Kappa 0.6167955252706157, Accuracy 0.8116169544740973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 56.70999043714255, Val Loss 49.7657524170354, Val Accuracy 0.8171114599686028, Precision 0.8720189684404396, Recall 0.5640846391417892, F1 0.5667039030050876, Cohen Kappa 0.6263139475453258, Accuracy 0.8171114599686028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 55.579563707113266, Val Loss 50.76254848949611, Val Accuracy 0.8186813186813187, Precision 0.6754970051856044, Recall 0.5727192188965112, F1 0.5836235153591248, Cohen Kappa 0.6275989831207665, Accuracy 0.8186813186813187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 47.91021048370749, Val Loss 59.59460286051035, Val Accuracy 0.7959183673469388, Precision 0.8589915216421241, Recall 0.5480917016350088, F1 0.5339533137890643, Cohen Kappa 0.5938441467036316, Accuracy 0.7959183673469388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 44.12384212203324, Val Loss 57.25195435248315, Val Accuracy 0.8296703296703297, Precision 0.7197400699410199, Recall 0.5752656781358838, F1 0.59033250825002, Cohen Kappa 0.6446596376134466, Accuracy 0.8296703296703297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 57.451439428608865, Val Loss 53.72981924563646, Val Accuracy 0.8061224489795918, Precision 0.595795214270824, Recall 0.554475657345863, F1 0.5658447803401551, Cohen Kappa 0.5935395787344466, Accuracy 0.8061224489795918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 37.98038768582046, Val Loss 54.32605626806617, Val Accuracy 0.8139717425431711, Precision 0.5994759969138395, Recall 0.578876457591215, F1 0.5859698494599497, Cohen Kappa 0.621268509285217, Accuracy 0.8139717425431711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 39.12367732566781, Val Loss 58.59046542271972, Val Accuracy 0.8218210361067504, Precision 0.8787089080545311, Recall 0.5711467803161986, F1 0.5871787992882935, Cohen Kappa 0.6291948266422538, Accuracy 0.8218210361067504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 34.64860562188551, Val Loss 62.66008293163031, Val Accuracy 0.8194662480376766, Precision 0.7631051229829233, Recall 0.5744344410841743, F1 0.5860550750219544, Cohen Kappa 0.6302362792951497, Accuracy 0.8194662480376766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 29.713815464871004, Val Loss 64.47858343878761, Val Accuracy 0.8163265306122449, Precision 0.6349834151453585, Recall 0.5742179920488276, F1 0.5811356958047601, Cohen Kappa 0.6272953672587613, Accuracy 0.8163265306122449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 28.646720999618992, Val Loss 65.25784220639616, Val Accuracy 0.8241758241758241, Precision 0.7139996987139415, Recall 0.5623994287951616, F1 0.5689554546174064, Cohen Kappa 0.6340953355412549, Accuracy 0.8241758241758241\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        " \n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['a'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        " \n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader  \n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        " \n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers).to(device)  # Input dim is 768\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    # new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    # evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    # print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    # evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_A_BLSTM.csv', index=False)\n",
        "    # scheduler.step()\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_TORCH/eva_A_BLSTM.csv', index=False)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_TORCH/model-A')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "1f1fc22f25b7ff86ab71b39773a0fc35f3d0ff49e86d16dc027a0214df64a378"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03fb54651ae0420890eeb25a410becba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b7cbcccd47241c4b2ef7a33ee9dc5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "132c2d61a02f4751bcd99870a7e284ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c3fdb9819e48c682527ef715f7b087": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fef5586ac0b411dafe680263fcb287b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2626087347db4e58a8cd54feb2beeadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28320915ba4344fe9a54071dfbb726d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7308d4b0adb34486bc5e4e58f0375d09",
              "IPY_MODEL_74a5028ceb7c40efbc76e03d974ed070",
              "IPY_MODEL_bffe7ba8f11642faa0e36dabc60c8836"
            ],
            "layout": "IPY_MODEL_6e01005ae3a740fda7a006fb78ba8740"
          }
        },
        "28e56b6bd3424b1eba20ee274de2961c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d2ce78ef8294030b716d08151182393",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4eb593500d643c9aba800b3a9d03be8",
            "value": 570
          }
        },
        "2c450c32aa774c9ebe2bf35c73bf4eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eefbfad15ce540c4b9c63c86c48d5d9a",
            "placeholder": "",
            "style": "IPY_MODEL_2c60934f7821478aa0dac546dea2b57f",
            "value": "232k/232k[00:00&lt;00:00,580kB/s]"
          }
        },
        "2c60934f7821478aa0dac546dea2b57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bf7f64ccf94442294d478b69f09fcf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5248e82da44eb8afcdfaf262f725ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47e2523eae8f4ba9bc0cc740b98fc84e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cdad662934b4a8aa1df0c7b01d8fec8",
            "placeholder": "",
            "style": "IPY_MODEL_cdc177ddd3a448b4befa3f9d3452b01b",
            "value": "model.safetensors:100%"
          }
        },
        "52d51502b6a14d1db2d6be60dfe0dc54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f2073b5c424473bcb2277323549e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a0e6dec05f457ba62169de5d403816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fe771f29b644fe48389be1c9ae56670",
              "IPY_MODEL_7824678ff8b640689489182b7fdf44db",
              "IPY_MODEL_bb059786d42543ce9c9284766291e568"
            ],
            "layout": "IPY_MODEL_cd9b8d9f37964e689938b8e9ffa31366"
          }
        },
        "59c20e1ae2a847debda8a950d35a1704": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c705fa24849a4bac88104f28fc51d2d1",
              "IPY_MODEL_28e56b6bd3424b1eba20ee274de2961c",
              "IPY_MODEL_e15ab9dc43ab420e9ffed8f0d5df5951"
            ],
            "layout": "IPY_MODEL_13c3fdb9819e48c682527ef715f7b087"
          }
        },
        "5a3a9c81442d40deb9f6c0fd865d9411": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "655d961e6134473a95210a6b91bf9b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0113afd6e2436d8271e1be124c7151": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e01005ae3a740fda7a006fb78ba8740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7308d4b0adb34486bc5e4e58f0375d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a3a9c81442d40deb9f6c0fd865d9411",
            "placeholder": "",
            "style": "IPY_MODEL_3bf7f64ccf94442294d478b69f09fcf0",
            "value": "tokenizer.json:100%"
          }
        },
        "74284f8722474e41891e99edf21c0df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74a5028ceb7c40efbc76e03d974ed070": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d51502b6a14d1db2d6be60dfe0dc54",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fef5586ac0b411dafe680263fcb287b",
            "value": 466062
          }
        },
        "7730ac7fad314ff08180154ff27de8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7824678ff8b640689489182b7fdf44db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_874e6f689a894aeeb27c7dfa13d91099",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2626087347db4e58a8cd54feb2beeadd",
            "value": 48
          }
        },
        "83b4594b9629415984c2e0e12887564e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "874e6f689a894aeeb27c7dfa13d91099": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cdad662934b4a8aa1df0c7b01d8fec8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932b21d80dae4372ad6b5ac1421fa828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a94e22b799f4e2b86b183d0b2855abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5248e82da44eb8afcdfaf262f725ce",
            "placeholder": "",
            "style": "IPY_MODEL_be1cade7dac14367a7d165b92e23c1e0",
            "value": "440M/440M[00:06&lt;00:00,63.4MB/s]"
          }
        },
        "9d2ce78ef8294030b716d08151182393": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fe771f29b644fe48389be1c9ae56670": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_655d961e6134473a95210a6b91bf9b9f",
            "placeholder": "",
            "style": "IPY_MODEL_83b4594b9629415984c2e0e12887564e",
            "value": "tokenizer_config.json:100%"
          }
        },
        "a3a7ecbac6674be9912d4e626b961108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b99433b11a064605828d104a12c40f4a",
              "IPY_MODEL_b130dd7c94ac403c922dd620741ad366",
              "IPY_MODEL_2c450c32aa774c9ebe2bf35c73bf4eed"
            ],
            "layout": "IPY_MODEL_b42c9283f69b4201815620fc5acdde2f"
          }
        },
        "b130dd7c94ac403c922dd620741ad366": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f2073b5c424473bcb2277323549e3a",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03fb54651ae0420890eeb25a410becba",
            "value": 231508
          }
        },
        "b42c9283f69b4201815620fc5acdde2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b705ffe3c8004594aa1dbd4a0aab4288": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99433b11a064605828d104a12c40f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b7cbcccd47241c4b2ef7a33ee9dc5ea",
            "placeholder": "",
            "style": "IPY_MODEL_74284f8722474e41891e99edf21c0df2",
            "value": "vocab.txt:100%"
          }
        },
        "bb059786d42543ce9c9284766291e568": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec95d06874447a3a27a0753b5026043",
            "placeholder": "",
            "style": "IPY_MODEL_c412c019398a4570930da3fbf7b0a5be",
            "value": "48.0/48.0[00:00&lt;00:00,2.07kB/s]"
          }
        },
        "bd2f8f636f8e40909bece34fc2bae374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be1cade7dac14367a7d165b92e23c1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bffe7ba8f11642faa0e36dabc60c8836": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7730ac7fad314ff08180154ff27de8a5",
            "placeholder": "",
            "style": "IPY_MODEL_bd2f8f636f8e40909bece34fc2bae374",
            "value": "466k/466k[00:00&lt;00:00,781kB/s]"
          }
        },
        "c2d78b2ca116425997345bfc24dfd4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c412c019398a4570930da3fbf7b0a5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4eb593500d643c9aba800b3a9d03be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c705fa24849a4bac88104f28fc51d2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0113afd6e2436d8271e1be124c7151",
            "placeholder": "",
            "style": "IPY_MODEL_c2d78b2ca116425997345bfc24dfd4f6",
            "value": "config.json:100%"
          }
        },
        "cd9b8d9f37964e689938b8e9ffa31366": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc177ddd3a448b4befa3f9d3452b01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc8b98e49ff4d08a48e4868d4abe3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47e2523eae8f4ba9bc0cc740b98fc84e",
              "IPY_MODEL_ddb1fdd546cd409ebbc25f50def745b3",
              "IPY_MODEL_9a94e22b799f4e2b86b183d0b2855abe"
            ],
            "layout": "IPY_MODEL_eb23ef4a723a40b199e87edc74434198"
          }
        },
        "ddb1fdd546cd409ebbc25f50def745b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_132c2d61a02f4751bcd99870a7e284ea",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_932b21d80dae4372ad6b5ac1421fa828",
            "value": 440449768
          }
        },
        "e15ab9dc43ab420e9ffed8f0d5df5951": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b705ffe3c8004594aa1dbd4a0aab4288",
            "placeholder": "",
            "style": "IPY_MODEL_f62621a2f6e64d898f829fd71aab6646",
            "value": "570/570[00:00&lt;00:00,6.86kB/s]"
          }
        },
        "eb23ef4a723a40b199e87edc74434198": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eefbfad15ce540c4b9c63c86c48d5d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62621a2f6e64d898f829fd71aab6646": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec95d06874447a3a27a0753b5026043": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
