{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# This will make only the GPUs with index 4 and 5 visible to PyTorch\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '3, 4'\n",
        "\n",
        "# Check if CUDA is available and then set the device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')  # This will use the first available GPU in the list, i.e., GPU 4\n",
        "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Using CPU')\n",
        "\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363,
          "referenced_widgets": [
            "94dbdd7e977b4e66b674592f1e48c53b",
            "4cac51aa9c744bee97a09dfe15a630e3",
            "c8fb9f2f59f54f0691a6e0a28869e556",
            "52ccf50e0cf6427c8ed3f251dd96405b",
            "cfa849b7d9de4510826fda6c506d928d",
            "12fac696c631479db823806b69fb07eb",
            "bbd250209a784cf1996ee289d0692aed",
            "6f6d391d4e2a434d9d05ff73ccca43f3",
            "7cbcbb0bc28a41d68804f74901aaf677",
            "1f86d7fda26147c68ea723b810388fd8",
            "83bc0befebbf4842982e8b4a38e7f07d",
            "c892f76d0a344822a0649ad81d4ff920",
            "a38724c034c2479dac4d1d659f5238e3",
            "f46afb0540384a19b5955715aa49721b",
            "027c3ccd1f534c9198c345413d928c05",
            "3abb2234b9474682bf3764d555b82660",
            "78ee13f761de4fe986de320730542b55",
            "07571df3674a45f581cc2b555439c60c",
            "f81b3033d583413d96e8b51ddc5a445d",
            "4b8ba04d004b4624950c03311629d1ac",
            "72266fdc9c5a43d4ae7408b3e73715e4",
            "9ebe3a01e76444ffbc5a57aed228ac54",
            "5d61134be7404122b424cf540c0bdd1d",
            "f84b04465d104da783ed7205e76e2bc1",
            "31f03477178d4e52a241c364f3ca9645",
            "5dc0d027289449abb25e9bffb8089927",
            "d851b8c85fe3472b8e123302ac79213d",
            "1ff88f9f79cb415aaa45deb82d14ef4f",
            "490ea7edeb3644c98f5d80e4c97c8eeb",
            "55160331a9e44a4aa05a1447dd430709",
            "1919ef6fa05c4feda2c8764a070af4a2",
            "cea9deb568a043819b2dd05cac85df34",
            "2ad0409c1db3441685ed851cbede81da",
            "7826ddf934b6446f8b3fbebfb0aa3d5e",
            "f5fecc3c4e154dbfb7b9ac1d776fdd7e",
            "83387cb3fb364845aafc2d59afbe5beb",
            "3055afcd834d4153a5217f5ebad676a9",
            "7404d7c9c6fc4c728b56f7172d080b48",
            "6d39cd82cbd24b78b7bd3181e227d30b",
            "5e33156da7be42f89533f6b650f702ad",
            "7dea267805064184a394d924cef06e4d",
            "97693fb687c8418899da26c59eee2b87",
            "4564813240954db5b8ab9d52c2203e74",
            "627db4cc35ad4c9ab1f51fb924e276e3",
            "0da5223538714fa8bd96a05bef5f4714",
            "714dc8f6b26b4781bff8db0aa57f4b38",
            "d6ed21b8f9644662abdf1d526e49a3b5",
            "1da18aec495444989fe705b6e34e7f7c",
            "2e47a725420f4bc49cbd65d8c25fa834",
            "bb5f890586d544d2958336f144fc8394",
            "28b6dec78c5449e98e2f221675edac28",
            "89d9191237164b988943d82af185189b",
            "e10572a311264ae4a1269e2684f65846",
            "15ff7af9a7d74f01bb7f565f821322e3",
            "5b9b2447d15b4bc0b4386c9e678324a7"
          ]
        },
        "id": "PkANHSTqXQWg",
        "outputId": "07612834-c348-40b5-ccb9-da46fe1d5857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "df = pd.read_csv('/home/leili/Vul/LLMDatasets/Datasets/CSDatasets/formatted_Sample_Original_0_6370.csv')\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True).to(device)\n",
        "#model = model.half()\n",
        "\n",
        "def batch_process(texts, layer_index):\n",
        "\n",
        "    tokens = tokenizer(texts, add_special_tokens=True, max_length=120, padding='max_length', truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "    layer_output = hidden_states[layer_index].detach().to('cpu')\n",
        "    return layer_output.numpy()\n",
        "\n",
        "batch_size = 100\n",
        "results = []\n",
        "\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch_texts = df['text'][i:i + batch_size].tolist()\n",
        "    batch_output = batch_process(batch_texts,12)\n",
        "    results.append(batch_output)\n",
        "\n",
        "results = np.vstack(results)\n",
        "\n",
        "\n",
        "expected_shape = (len(df), 120, 768)\n",
        "if results.shape == expected_shape:\n",
        "    df['layer_12_output'] = list(results)\n",
        "else:\n",
        "    print(f\"Unexpected result shape: {results.shape}. Expected: {expected_shape}\")\n",
        "\n",
        "df['av'] = df['av'].map({'N': 0, 'L': 1, 'A': 2, 'P': 3})  # Adjust mapping as needed\n",
        "df['ac'] = df['ac'].map({'L': 0, 'H': 1})\n",
        "df['pr'] = df['pr'].map({'L': 0, 'H': 1, 'N': 2})\n",
        "df['ui'] = df['ui'].map({'R': 0, 'N': 1})\n",
        "df['s'] = df['s'].map({'C': 0, 'U': 1})\n",
        "df['c'] = df['c'].map({'H': 0, 'N': 1, 'L': 2})\n",
        "df['i'] = df['i'].map({'H': 0, 'N': 1, 'L': 2})\n",
        "df['a'] = df['a'].map({'H': 0, 'N': 1, 'L': 2})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "daTqjBjOXcuf",
        "outputId": "1bb09c59-e591-47e7-b0d8-aa78012c59eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>vectorString</th>\n",
              "      <th>av</th>\n",
              "      <th>ac</th>\n",
              "      <th>pr</th>\n",
              "      <th>ui</th>\n",
              "      <th>s</th>\n",
              "      <th>c</th>\n",
              "      <th>i</th>\n",
              "      <th>a</th>\n",
              "      <th>score</th>\n",
              "      <th>baseSeverity</th>\n",
              "      <th>layer_12_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CVE-2016-0002</td>\n",
              "      <td>The Microsoft (1) VBScript 5.7 and 5.8 and (2)...</td>\n",
              "      <td>CVSS:3.0/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.5</td>\n",
              "      <td>HIGH</td>\n",
              "      <td>[[-0.18392727, -0.2513095, -0.39242408, -0.213...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CVE-2016-0003</td>\n",
              "      <td>Microsoft Edge allows remote attackers to exec...</td>\n",
              "      <td>CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>CRITICAL</td>\n",
              "      <td>[[-0.34586793, -0.48593235, -0.6668904, -0.174...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              ID                                               text  \\\n",
              "0  CVE-2016-0002  The Microsoft (1) VBScript 5.7 and 5.8 and (2)...   \n",
              "1  CVE-2016-0003  Microsoft Edge allows remote attackers to exec...   \n",
              "\n",
              "                                   vectorString  av  ac  pr  ui  s  c  i  a  \\\n",
              "0  CVSS:3.0/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:H/A:H   0   1   2   0  1  0  0  0   \n",
              "1  CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H   0   0   2   0  0  0  0  0   \n",
              "\n",
              "   score baseSeverity                                    layer_12_output  \n",
              "0    7.5         HIGH  [[-0.18392727, -0.2513095, -0.39242408, -0.213...  \n",
              "1    9.6     CRITICAL  [[-0.34586793, -0.48593235, -0.6668904, -0.174...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJSf1z9wOZpv",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 240.86007237434387, Val Loss 47.80671486258507, Val Accuracy 0.750392464678179, Precision 0.9072817836812144, Recall 0.273702484222968, F1 0.2575800721395059, Cohen Kappa 0.12063935821076932, Accuracy 0.750392464678179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 171.33913286030293, Val Loss 38.72560325264931, Val Accuracy 0.804552590266876, Precision 0.8772062592094305, Recall 0.3651397947051085, F1 0.3685084512966348, Cohen Kappa 0.46259842519685046, Accuracy 0.804552590266876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 151.29657326638699, Val Loss 36.48327735066414, Val Accuracy 0.826530612244898, Precision 0.8872644639065818, Recall 0.39897334965561393, F1 0.3930305333082874, Cohen Kappa 0.5567139834244401, Accuracy 0.826530612244898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 140.894784912467, Val Loss 35.758096389472485, Val Accuracy 0.826530612244898, Precision 0.9080180127916926, Recall 0.36719590958019377, F1 0.3775815929373457, Cohen Kappa 0.4958954241812782, Accuracy 0.826530612244898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 127.08933366090059, Val Loss 35.16737721115351, Val Accuracy 0.826530612244898, Precision 0.9086618876941457, Recall 0.3666480226823417, F1 0.37721956406166934, Cohen Kappa 0.4947683109118086, Accuracy 0.826530612244898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 118.43152918666601, Val Loss 32.93648990243673, Val Accuracy 0.8312401883830455, Precision 0.8896959584723767, Recall 0.4192161435797857, F1 0.4025302281045059, Cohen Kappa 0.593095200956689, Accuracy 0.8312401883830455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 109.19663005322218, Val Loss 34.72080156207085, Val Accuracy 0.8414442700156985, Precision 0.9168327861163227, Recall 0.38217089849933517, F1 0.39215694879221397, Cohen Kappa 0.547809401982731, Accuracy 0.8414442700156985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 101.51797723770142, Val Loss 32.80988400056958, Val Accuracy 0.8524332810047096, Precision 0.6568372570871353, Recall 0.4111411877273335, F1 0.6587413018554584, Cohen Kappa 0.6155363681163188, Accuracy 0.8524332810047096\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 95.9530519284308, Val Loss 31.80978797376156, Val Accuracy 0.8571428571428571, Precision 0.6636970825349771, Recall 0.4116600533288306, F1 0.6619859453763564, Cohen Kappa 0.6221383500208594, Accuracy 0.8571428571428571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 86.65211495757103, Val Loss 32.71500355750322, Val Accuracy 0.8485086342229199, Precision 0.7748160429878491, Recall 0.43896369096616045, F1 0.4337440216022441, Cohen Kappa 0.6319781834945324, Accuracy 0.8485086342229199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 76.40818950720131, Val Loss 34.790993828326464, Val Accuracy 0.8516483516483516, Precision 0.7305736123101925, Recall 0.45686681561752795, F1 0.4661023589647274, Cohen Kappa 0.6195584852238543, Accuracy 0.8516483516483516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 71.27161411941051, Val Loss 33.14989422634244, Val Accuracy 0.8563579277864992, Precision 0.47380544309295, Recall 0.46286455019754585, F1 0.7176051239209134, Cohen Kappa 0.635687163059614, Accuracy 0.8563579277864992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 64.36443016957492, Val Loss 37.93122571520507, Val Accuracy 0.8532182103610675, Precision 0.727039196116681, Recall 0.44509145602984646, F1 0.44891116984774326, Cohen Kappa 0.6339603566396713, Accuracy 0.8532182103610675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 57.84961910825223, Val Loss 37.88727206736803, Val Accuracy 0.8610675039246468, Precision 0.7515950520833333, Recall 0.43969424870773566, F1 0.46041592815786364, Cohen Kappa 0.6229950010867202, Accuracy 0.8610675039246468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 49.39298077812418, Val Loss 38.56087386608124, Val Accuracy 0.8634222919937206, Precision 0.7360419649510604, Recall 0.47890629161603643, F1 0.49995202311568687, Cohen Kappa 0.6497245845065749, Accuracy 0.8634222919937206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 48.090275899041444, Val Loss 40.138848344795406, Val Accuracy 0.8563579277864992, Precision 0.4500794903253601, Recall 0.4335664836440497, F1 0.6904425423668834, Cohen Kappa 0.6268014272153772, Accuracy 0.8563579277864992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 41.393754756310955, Val Loss 42.3236339725554, Val Accuracy 0.8555729984301413, Precision 0.708659906774882, Recall 0.5045505643951158, F1 0.5318896648280282, Cohen Kappa 0.6378092664438666, Accuracy 0.8555729984301413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 39.78844854957424, Val Loss 44.95394757203758, Val Accuracy 0.8461538461538461, Precision 0.6072357917848448, Recall 0.462786531907345, F1 0.48457312004895275, Cohen Kappa 0.6126530283005405, Accuracy 0.8461538461538461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 37.0819619812537, Val Loss 41.57525905221701, Val Accuracy 0.8532182103610675, Precision 0.5763061785348773, Recall 0.48780188429875, F1 0.5007490380603588, Cohen Kappa 0.6422938788257339, Accuracy 0.8532182103610675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 31.17457620287314, Val Loss 46.684604002162814, Val Accuracy 0.8367346938775511, Precision 0.501620485909698, Recall 0.5257494278658719, F1 0.5123346555781968, Cohen Kappa 0.6203172237903514, Accuracy 0.8367346938775511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 36.5998383811675, Val Loss 41.468555034603924, Val Accuracy 0.858712715855573, Precision 0.6054109535982648, Recall 0.49884492727841045, F1 0.534700854220205, Cohen Kappa 0.6304878166683318, Accuracy 0.858712715855573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 30.323404324939474, Val Loss 47.16862076707184, Val Accuracy 0.859497645211931, Precision 0.556873103658039, Recall 0.4816064028769025, F1 0.49604368714867475, Cohen Kappa 0.6575623658868772, Accuracy 0.859497645211931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 26.681131474208087, Val Loss 52.00299493782222, Val Accuracy 0.8579277864992151, Precision 0.6512213505614521, Recall 0.4735278036582412, F1 0.5069769393291933, Cohen Kappa 0.6302070637741028, Accuracy 0.8579277864992151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 28.137705329572782, Val Loss 47.95481817377731, Val Accuracy 0.8500784929356358, Precision 0.5732156936377879, Recall 0.4878212318296533, F1 0.49894172407885556, Cohen Kappa 0.6380283408157134, Accuracy 0.8500784929356358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 24.56151540763676, Val Loss 47.67718601413071, Val Accuracy 0.8390894819466248, Precision 0.518592100406054, Recall 0.46537294501582477, F1 0.4671030483596761, Cohen Kappa 0.6223367488887155, Accuracy 0.8390894819466248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 25.086719720857218, Val Loss 60.3728951967787, Val Accuracy 0.8343799058084772, Precision 0.6545653088529663, Recall 0.4340176328873853, F1 0.48199431976200313, Cohen Kappa 0.5236674280266718, Accuracy 0.8343799058084772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 24.015853010001592, Val Loss 49.05002634972334, Val Accuracy 0.858712715855573, Precision 0.5998604438411729, Recall 0.5481299983215389, F1 0.5694336410180079, Cohen Kappa 0.6475204852203611, Accuracy 0.858712715855573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 19.219624149845913, Val Loss 60.275969431735575, Val Accuracy 0.8610675039246468, Precision 0.8103676561152852, Recall 0.4712395184123165, F1 0.5349146126092013, Cohen Kappa 0.608351989606907, Accuracy 0.8610675039246468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 15.46244648087304, Val Loss 53.91950921295211, Val Accuracy 0.8547880690737834, Precision 0.6055114725250001, Recall 0.4935341556787768, F1 0.5255532533927132, Cohen Kappa 0.6313859195020293, Accuracy 0.8547880690737834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 20.776735606137663, Val Loss 48.779608068522066, Val Accuracy 0.8579277864992151, Precision 0.6152219763645104, Recall 0.4905649378517105, F1 0.5277137411661609, Cohen Kappa 0.6206075044669446, Accuracy 0.8579277864992151\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['av'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_AV_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-AV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94Ww_aJJQKkw",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 93.14135956019163, Val Loss 22.723089180886745, Val Accuracy 0.9089481946624803, Precision 0.9544740973312402, Recall 0.5, F1 0.4761513157894737, Cohen Kappa 0.0, Accuracy 0.9089481946624803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 75.86339389905334, Val Loss 20.910455867648125, Val Accuracy 0.9089481946624803, Precision 0.9544740973312402, Recall 0.5, F1 0.4761513157894737, Cohen Kappa 0.0, Accuracy 0.9089481946624803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 67.67705049365759, Val Loss 19.144678466022015, Val Accuracy 0.9191522762951334, Precision 0.9591593973037271, Recall 0.5560344827586207, F1 0.5794854046293716, Cohen Kappa 0.18662369057211925, Accuracy 0.9191522762951334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 59.14596193283796, Val Loss 17.964377036318183, Val Accuracy 0.9301412872841445, Precision 0.9327459580100174, Recall 0.6241364421416236, F1 0.6787449673747049, Cohen Kappa 0.3703785968925958, Accuracy 0.9301412872841445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 53.29358689766377, Val Loss 16.388082418590784, Val Accuracy 0.9356357927786499, Precision 0.8897712418300654, Recall 0.6775802513251147, F1 0.7357996216604453, Cohen Kappa 0.47735686698285007, Accuracy 0.9356357927786499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 48.47491136426106, Val Loss 16.314560221508145, Val Accuracy 0.9356357927786499, Precision 0.8726395730706076, Recall 0.6892159490203085, F1 0.7443720147208519, Cohen Kappa 0.49320836729149686, Accuracy 0.9356357927786499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 44.71906810812652, Val Loss 20.02982360497117, Val Accuracy 0.9246467817896389, Precision 0.9393334301481266, Recall 0.5900854624501221, F1 0.6322569005953455, Cohen Kappa 0.2835485155938796, Accuracy 0.9246467817896389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 40.545773279387504, Val Loss 17.308923962526023, Val Accuracy 0.9372056514913658, Precision 0.907320554649266, Recall 0.6784438091834911, F1 0.7393190374856768, Cohen Kappa 0.48473205257836205, Accuracy 0.9372056514913658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 35.191886618267745, Val Loss 18.009024757426232, Val Accuracy 0.9395604395604396, Precision 0.9121494877591265, Recall 0.6913748436662498, F1 0.7532914012378435, Cohen Kappa 0.5117705025730866, Accuracy 0.9395604395604396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 32.99756197491661, Val Loss 15.851322550326586, Val Accuracy 0.9387755102040817, Precision 0.856570068404511, Recall 0.729728723721041, F1 0.7760277677605482, Cohen Kappa 0.5540016516462605, Accuracy 0.9387755102040817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 29.80841460172087, Val Loss 18.972524435259402, Val Accuracy 0.9387755102040817, Precision 0.8759980836793356, Recall 0.7103358942290512, F1 0.7644431802019627, Cohen Kappa 0.532120458782982, Accuracy 0.9387755102040817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 27.25761356204748, Val Loss 17.141687165014446, Val Accuracy 0.9403453689167975, Precision 0.8713540142425129, Recall 0.7267137156810196, F1 0.7774038107849628, Cohen Kappa 0.5571533113794365, Accuracy 0.9403453689167975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 22.85675207944587, Val Loss 19.99242885550484, Val Accuracy 0.9340659340659341, Precision 0.846900826446281, Recall 0.6999880888571258, F1 0.7489301801801802, Cohen Kappa 0.5010257749263307, Accuracy 0.9340659340659341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 20.786941522033885, Val Loss 17.643763127271086, Val Accuracy 0.9317111459968603, Precision 0.8158307293045919, Recall 0.7219641474599487, F1 0.7584360390990224, Cohen Kappa 0.5183092715404473, Accuracy 0.9317111459968603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 17.583925090671983, Val Loss 22.768596813548356, Val Accuracy 0.9309262166405023, Precision 0.8153940778490467, Recall 0.7137752367339647, F1 0.7521576117713639, Cohen Kappa 0.5060623149584097, Accuracy 0.9309262166405023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 15.87517056753859, Val Loss 20.056983447400853, Val Accuracy 0.9364207221350078, Precision 0.8573200992555832, Recall 0.7090405574414864, F1 0.7591328277629561, Cohen Kappa 0.5211726384364821, Accuracy 0.9364207221350078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 17.806000065233093, Val Loss 20.026593449758366, Val Accuracy 0.9356357927786499, Precision 0.8272408963585434, Recall 0.7435158715978798, F1 0.7775383304940375, Cohen Kappa 0.5560447405997144, Accuracy 0.9356357927786499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 13.922342302510515, Val Loss 24.48405941668898, Val Accuracy 0.9270015698587127, Precision 0.7928287457133318, Recall 0.7154949079864213, F1 0.746536000564765, Cohen Kappa 0.494250224100397, Accuracy 0.9270015698587127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 12.05937738029752, Val Loss 23.936978162499145, Val Accuracy 0.9270015698587127, Precision 0.7903969245368891, Recall 0.7232520397832172, F1 0.7511065362840967, Cohen Kappa 0.5030909502679942, Accuracy 0.9270015698587127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 11.726957481878344, Val Loss 25.262259546667337, Val Accuracy 0.9183673469387755, Precision 0.7534709618874773, Recall 0.7495309987493299, F1 0.7514799114679072, Cohen Kappa 0.5029635520609816, Accuracy 0.9183673469387755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 9.625235800922383, Val Loss 27.247216180432588, Val Accuracy 0.934850863422292, Precision 0.8341451209341118, Recall 0.7236912631767018, F1 0.7651153853842958, Cohen Kappa 0.5319783653633363, Accuracy 0.934850863422292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 8.639055351231946, Val Loss 26.226128539419733, Val Accuracy 0.9136577708006279, Precision 0.7397256375227688, Recall 0.7508188910725984, F1 0.7450980392156863, Cohen Kappa 0.49022945858250766, Accuracy 0.9136577708006279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 9.817696145590162, Val Loss 27.216733944718726, Val Accuracy 0.9277864992150706, Precision 0.7940940411870645, Recall 0.7236838187124055, F1 0.7526693845855172, Cohen Kappa 0.5062764326273399, Accuracy 0.9277864992150706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 7.146381432074122, Val Loss 27.15531443542568, Val Accuracy 0.9301412872841445, Precision 0.8041804779102557, Recall 0.7288577213983681, F1 0.7596465902273888, Cohen Kappa 0.5202704417949348, Accuracy 0.9301412872841445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 7.886497916653752, Val Loss 27.870656433631666, Val Accuracy 0.9246467817896389, Precision 0.777465336728919, Recall 0.7335924006908463, F1 0.7530369604548979, Cohen Kappa 0.5064725441456566, Accuracy 0.9246467817896389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 8.835744739684742, Val Loss 28.24224217128358, Val Accuracy 0.934850863422292, Precision 0.8425942630344578, Recall 0.712055565481508, F1 0.7581132550239618, Cohen Kappa 0.518658788612631, Accuracy 0.934850863422292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 7.114611939105089, Val Loss 27.797167541953968, Val Accuracy 0.9419152276295133, Precision 0.88342865743528, Recall 0.727577273539396, F1 0.7810650887573963, Cohen Kappa 0.5646576405179261, Accuracy 0.9419152276295133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 4.132238918798976, Val Loss 28.936676302051637, Val Accuracy 0.9356357927786499, Precision 0.8338907469342252, Recall 0.731880173902686, F1 0.7712426315374576, Cohen Kappa 0.543927355278093, Accuracy 0.9356357927786499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 11.936896098748548, Val Loss 23.173911643680185, Val Accuracy 0.9215070643642073, Precision 0.7633390705679862, Recall 0.755136680364481, F1 0.7591500302480338, Cohen Kappa 0.5183146305314419, Accuracy 0.9215070643642073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 5.483324815024389, Val Loss 28.317825072444975, Val Accuracy 0.9230769230769231, Precision 0.768939393939394, Recall 0.7521216723244595, F1 0.7601656626506024, Cohen Kappa 0.5203902888752305, Accuracy 0.9230769230769231\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['ac'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_AC_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-AC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlVQQ9luQLBF",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 274.0319354534149, Val Loss 58.40074345469475, Val Accuracy 0.6679748822605965, Precision 0.7228346456692915, Recall 0.33448230839535187, F1 0.27067569630479216, Cohen Kappa 0.003741731771074064, Accuracy 0.6679748822605965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 232.52016647160053, Val Loss 49.334866374731064, Val Accuracy 0.7621664050235479, Precision 0.8087507051965147, Recall 0.5072986029507769, F1 0.4908355257150987, Cohen Kappa 0.4687487958074833, Accuracy 0.7621664050235479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 209.35737963020802, Val Loss 47.898807004094124, Val Accuracy 0.7668759811616954, Precision 0.8318735312967315, Recall 0.4723070896983941, F1 0.4719157148150635, Cohen Kappa 0.416717023480663, Accuracy 0.7668759811616954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 194.19724467396736, Val Loss 43.819637432694435, Val Accuracy 0.793563579277865, Precision 0.8366664579000052, Recall 0.5195195195195196, F1 0.5100259337486298, Cohen Kappa 0.5208636015238008, Accuracy 0.793563579277865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 182.9042520225048, Val Loss 43.15294460952282, Val Accuracy 0.793563579277865, Precision 0.8347861751297462, Recall 0.5252643948296122, F1 0.5126913027248974, Cohen Kappa 0.5283713100124431, Accuracy 0.793563579277865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 172.05754640698433, Val Loss 42.0218940526247, Val Accuracy 0.7974882260596546, Precision 0.837948044714962, Recall 0.5334026334026334, F1 0.5247126681725839, Cohen Kappa 0.538824823529907, Accuracy 0.7974882260596546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 160.46206928044558, Val Loss 41.216555163264275, Val Accuracy 0.8021978021978022, Precision 0.6751628001628002, Recall 0.5465284682675987, F1 0.5390643397897968, Cohen Kappa 0.5566578379736907, Accuracy 0.8021978021978022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 149.22849035263062, Val Loss 44.765320152044296, Val Accuracy 0.7849293563579278, Precision 0.6221840272012374, Recall 0.6138238573021182, F1 0.6169478988811555, Cohen Kappa 0.5491971903923714, Accuracy 0.7849293563579278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 141.4473908394575, Val Loss 41.70810156315565, Val Accuracy 0.8076923076923077, Precision 0.6945560550405997, Recall 0.5740054435706609, F1 0.5931002918505422, Cohen Kappa 0.5649956169263295, Accuracy 0.8076923076923077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 127.78602193295956, Val Loss 40.86370766162872, Val Accuracy 0.8108320251177394, Precision 0.6864542687355565, Recall 0.6360973013146927, F1 0.6559666644322332, Cohen Kappa 0.5866548734994339, Accuracy 0.8108320251177394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 119.15737736970186, Val Loss 41.638471871614456, Val Accuracy 0.8116169544740973, Precision 0.6902042970664732, Recall 0.6492833971094841, F1 0.666401043175891, Cohen Kappa 0.5918835875162507, Accuracy 0.8116169544740973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 107.72860389575362, Val Loss 43.33635565638542, Val Accuracy 0.7982731554160125, Precision 0.6730602133415929, Recall 0.6443640965380096, F1 0.6568016928860646, Cohen Kappa 0.5596622200465059, Accuracy 0.7982731554160125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 101.09090005978942, Val Loss 46.62641625478864, Val Accuracy 0.8076923076923077, Precision 0.6966191935907425, Recall 0.6671527715005977, F1 0.6776718673277164, Cohen Kappa 0.5971148918923455, Accuracy 0.8076923076923077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 90.36320373043418, Val Loss 45.52773775160313, Val Accuracy 0.8014128728414442, Precision 0.657080797576678, Recall 0.6597185814577119, F1 0.6582829390604606, Cohen Kappa 0.5829220636378929, Accuracy 0.8014128728414442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 87.8563670143485, Val Loss 49.1911863386631, Val Accuracy 0.8069073783359497, Precision 0.6896684687708531, Recall 0.6353038657386484, F1 0.6571856690450723, Cohen Kappa 0.568601648487785, Accuracy 0.8069073783359497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 80.83577097207308, Val Loss 51.280557706952095, Val Accuracy 0.7896389324960753, Precision 0.6667992025710459, Recall 0.6388903954121345, F1 0.6500814617704763, Cohen Kappa 0.5543611476931148, Accuracy 0.7896389324960753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 73.37226755172014, Val Loss 58.805758602917194, Val Accuracy 0.7880690737833596, Precision 0.6879050142985877, Recall 0.6770486205268814, F1 0.6763716325423865, Cohen Kappa 0.5733131554954631, Accuracy 0.7880690737833596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 65.02964386157691, Val Loss 52.010934460908175, Val Accuracy 0.7998430141287284, Precision 0.6764552674132869, Recall 0.6394026133156568, F1 0.6541369404748919, Cohen Kappa 0.5566101084215008, Accuracy 0.7998430141287284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 64.99833690188825, Val Loss 50.46532176434994, Val Accuracy 0.7888540031397174, Precision 0.6363471752776436, Recall 0.6544768849116674, F1 0.6389305781336483, Cohen Kappa 0.5565123092531628, Accuracy 0.7888540031397174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 56.70930903498083, Val Loss 59.828205277211964, Val Accuracy 0.7959183673469388, Precision 0.6682513998047979, Recall 0.6544156196330109, F1 0.6540623260678409, Cohen Kappa 0.5522870207299057, Accuracy 0.7959183673469388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 52.1359652816318, Val Loss 73.99270038306713, Val Accuracy 0.7378335949764521, Precision 0.5986374349614768, Recall 0.6590456677413199, F1 0.6185243629825292, Cohen Kappa 0.4961188996266328, Accuracy 0.7378335949764521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 48.25856670644134, Val Loss 55.43094730004668, Val Accuracy 0.8006279434850864, Precision 0.6688773745407, Recall 0.6596221639699901, F1 0.6636554597729437, Cohen Kappa 0.5735722201796659, Accuracy 0.8006279434850864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 46.290129194036126, Val Loss 64.13210585899651, Val Accuracy 0.804552590266876, Precision 0.7452383472173926, Recall 0.6300752257273996, F1 0.655037517690726, Cohen Kappa 0.5808889393871333, Accuracy 0.804552590266876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 42.46648231660947, Val Loss 60.83323465846479, Val Accuracy 0.8108320251177394, Precision 0.70361469239422, Recall 0.6698846003193829, F1 0.6849593439430101, Cohen Kappa 0.5912029153191394, Accuracy 0.8108320251177394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 41.428338560741395, Val Loss 65.00190914794803, Val Accuracy 0.7951334379905809, Precision 0.6774306826440055, Recall 0.6405154318197797, F1 0.6500294887624478, Cohen Kappa 0.5399283840474636, Accuracy 0.7951334379905809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 36.107531134039164, Val Loss 73.44228794425726, Val Accuracy 0.7543171114599686, Precision 0.6143881026207056, Recall 0.6530326312935009, F1 0.6294542892846241, Cohen Kappa 0.5155528666258468, Accuracy 0.7543171114599686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 31.619493409525603, Val Loss 72.5220339000225, Val Accuracy 0.7613814756671899, Precision 0.6088104929139305, Recall 0.6516827864653951, F1 0.6237253617405828, Cohen Kappa 0.5217398820946355, Accuracy 0.7613814756671899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 31.721403452102095, Val Loss 67.489725086838, Val Accuracy 0.8006279434850864, Precision 0.6850691818971798, Recall 0.656314843271365, F1 0.6693001161561, Cohen Kappa 0.5694577308971117, Accuracy 0.8006279434850864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 29.68932718387805, Val Loss 64.14590369351208, Val Accuracy 0.7982731554160125, Precision 0.6624703814109913, Recall 0.6604658169875561, F1 0.6591632249300255, Cohen Kappa 0.5676143236718804, Accuracy 0.7982731554160125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 33.33580412645824, Val Loss 65.21563787199557, Val Accuracy 0.8006279434850864, Precision 0.7054559397314892, Recall 0.6532867315476012, F1 0.672889338058139, Cohen Kappa 0.5735019664491097, Accuracy 0.8006279434850864\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['pr'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_PR_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-PR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0gqNTGdQLPb",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 186.31492403149605, Val Loss 40.98573596030474, Val Accuracy 0.7723704866562009, Precision 0.7969282307517602, Recall 0.6974460625674218, F1 0.7112871967991998, Cohen Kappa 0.44216622981313947, Accuracy 0.7723704866562009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 134.35899336636066, Val Loss 29.147422030568123, Val Accuracy 0.8367346938775511, Precision 0.8376998115101222, Recall 0.7976348435814455, F1 0.8112662493981704, Cohen Kappa 0.6248931267021873, Accuracy 0.8367346938775511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 113.34967108815908, Val Loss 26.276757404208183, Val Accuracy 0.8673469387755102, Precision 0.8672026326508485, Recall 0.8379422869471413, F1 0.8492882867432687, Cohen Kappa 0.6995207538427615, Accuracy 0.8673469387755102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 99.571778152138, Val Loss 27.741057854145765, Val Accuracy 0.8673469387755102, Precision 0.8783164253078293, Recall 0.8293689320388349, F1 0.8459294816061529, Cohen Kappa 0.6940920056945893, Accuracy 0.8673469387755102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 90.84407077357173, Val Loss 24.946416676044464, Val Accuracy 0.8775510204081632, Precision 0.880221947486165, Recall 0.8483522114347357, F1 0.8606224665834467, Cohen Kappa 0.7221995471191748, Accuracy 0.8775510204081632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 79.65309575386345, Val Loss 23.801252841949463, Val Accuracy 0.8869701726844584, Precision 0.8927670236459271, Recall 0.857651024811219, F1 0.8710210830921868, Cohen Kappa 0.7430314711369838, Accuracy 0.8869701726844584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 73.80193253979087, Val Loss 23.362174481153488, Val Accuracy 0.8924646781789639, Precision 0.8958265966341266, Recall 0.8664374325782093, F1 0.8781250283672939, Cohen Kappa 0.7569237282706487, Accuracy 0.8924646781789639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 65.86793884355575, Val Loss 24.399036716669798, Val Accuracy 0.8901098901098901, Precision 0.9023674933138828, Recall 0.8570523193096009, F1 0.8733040673968944, Cohen Kappa 0.748057745006639, Accuracy 0.8901098901098901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 56.9589475877583, Val Loss 26.75284083932638, Val Accuracy 0.8893249607535322, Precision 0.8892822163949117, Recall 0.8655231930960087, F1 0.8753188450158147, Cohen Kappa 0.7511229214523025, Accuracy 0.8893249607535322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 55.32548033632338, Val Loss 26.81278452835977, Val Accuracy 0.8830455259026687, Precision 0.8732159530478858, Recall 0.8697464940668824, F1 0.8714253105723635, Cohen Kappa 0.7428647285278109, Accuracy 0.8830455259026687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 47.68312448542565, Val Loss 25.34789922647178, Val Accuracy 0.8971742543171115, Precision 0.8947921159989061, Recall 0.8776429341963323, F1 0.8850993489188004, Cohen Kappa 0.7704390837040277, Accuracy 0.8971742543171115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 44.91376730846241, Val Loss 30.497132832184434, Val Accuracy 0.880690737833595, Precision 0.8702024313470096, Recall 0.8679261057173678, F1 0.8690396002910712, Cohen Kappa 0.7380855771779765, Accuracy 0.880690737833595\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 40.35839693620801, Val Loss 26.1822041682899, Val Accuracy 0.8830455259026687, Precision 0.8769626931290039, Recall 0.8641990291262136, F1 0.8698976252372261, Cohen Kappa 0.7399665208231165, Accuracy 0.8830455259026687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 38.23924186779186, Val Loss 30.17887445213273, Val Accuracy 0.8963893249607535, Precision 0.9055056862885345, Recall 0.8669498381877023, F1 0.8814693269954581, Cohen Kappa 0.7639508834576968, Accuracy 0.8963893249607535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 31.761077200993896, Val Loss 26.06741509400308, Val Accuracy 0.8963893249607535, Precision 0.8946219942942535, Recall 0.8760275080906148, F1 0.8840236686390532, Cohen Kappa 0.7683291729348167, Accuracy 0.8963893249607535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 28.5062947133556, Val Loss 35.292182471370324, Val Accuracy 0.8940345368916798, Precision 0.8976822841561574, Recall 0.8681553398058253, F1 0.8799042250334647, Cohen Kappa 0.7604722869820261, Accuracy 0.8940345368916798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 25.68423730158247, Val Loss 33.16071690432727, Val Accuracy 0.8759811616954474, Precision 0.8638417320296551, Recall 0.8652939590075512, F1 0.8645570914884912, Cohen Kappa 0.7291170992711574, Accuracy 0.8759811616954474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 23.23684598156251, Val Loss 33.998928285669535, Val Accuracy 0.8995290423861853, Precision 0.9038624886551395, Recall 0.8744201725997842, F1 0.8862007843794052, Cohen Kappa 0.7730104730990017, Accuracy 0.8995290423861853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 23.0450093569234, Val Loss 35.680923430714756, Val Accuracy 0.890894819466248, Precision 0.8867091181957165, Recall 0.8717799352750809, F1 0.8783578731895629, Cohen Kappa 0.7569202893577301, Accuracy 0.890894819466248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 19.948090920690447, Val Loss 37.49830280325841, Val Accuracy 0.8956043956043956, Precision 0.8874101455403545, Recall 0.8829854368932039, F1 0.8851124622080468, Cohen Kappa 0.7702437741191019, Accuracy 0.8956043956043956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 19.874770352151245, Val Loss 38.852124421158805, Val Accuracy 0.8830455259026687, Precision 0.8754620937929433, Recall 0.8662162891046385, F1 0.8704672299000933, Cohen Kappa 0.7410279427774701, Accuracy 0.8830455259026687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 17.984490839298815, Val Loss 36.86641167663038, Val Accuracy 0.8861852433281004, Precision 0.8755794020381993, Recall 0.8751995685005394, F1 0.8753887986854203, Cohen Kappa 0.7507777654858303, Accuracy 0.8861852433281004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 13.491472041117959, Val Loss 38.27178458496928, Val Accuracy 0.8885400313971743, Precision 0.8870698114929938, Recall 0.8659250269687162, F1 0.8748034591194969, Cohen Kappa 0.7500055275493057, Accuracy 0.8885400313971743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 18.180927323759533, Val Loss 37.385404490400106, Val Accuracy 0.8940345368916798, Precision 0.8964802623004, Recall 0.8691639697950377, F1 0.8801953486622547, Cohen Kappa 0.7609702946629346, Accuracy 0.8940345368916798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 13.352715987653937, Val Loss 40.16983049281407, Val Accuracy 0.8916797488226059, Precision 0.8934240362811792, Recall 0.866839266450917, F1 0.8776067700138954, Cohen Kappa 0.755785461475633, Accuracy 0.8916797488226059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 11.064348646148574, Val Loss 51.35223847348243, Val Accuracy 0.8720565149136578, Precision 0.8573804763958031, Recall 0.8688160733549083, F1 0.8623072562170334, Cohen Kappa 0.7248380129589633, Accuracy 0.8720565149136578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 11.279936358390842, Val Loss 45.962421023170464, Val Accuracy 0.8838304552590267, Precision 0.8821138443935926, Recall 0.8602669902912621, F1 0.8693604933139334, Cohen Kappa 0.7391727763176097, Accuracy 0.8838304552590267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 10.328137150325347, Val Loss 42.270556335453875, Val Accuracy 0.8885400313971743, Precision 0.8821920088790233, Recall 0.8714724919093851, F1 0.876350920936438, Cohen Kappa 0.7528160508064208, Accuracy 0.8885400313971743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 12.386633351619821, Val Loss 45.8181777218706, Val Accuracy 0.8885400313971743, Precision 0.882581825868322, Recall 0.8709681769147788, F1 0.8762152131549561, Cohen Kappa 0.7525631626746616, Accuracy 0.8885400313971743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 10.01800802059006, Val Loss 50.305223201430636, Val Accuracy 0.8932496075353218, Precision 0.907869452400186, Recall 0.8594795037756202, F1 0.8765971630680345, Cohen Kappa 0.7547378136129685, Accuracy 0.8932496075353218\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['ui'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_UI_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-UI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51nNAYtIQLd4",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 133.7518193423748, Val Loss 24.628247261047363, Val Accuracy 0.8783359497645212, Precision 0.8668460468161728, Recall 0.658664206413473, F1 0.7028895559216713, Cohen Kappa 0.4207358212720521, Accuracy 0.8783359497645212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 83.88020888715982, Val Loss 18.06329655647278, Val Accuracy 0.9277864992150706, Precision 0.9338636057238603, Recall 0.7990468731332386, F1 0.8476433121019109, Cohen Kappa 0.6975574008226291, Accuracy 0.9277864992150706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 65.7301595825702, Val Loss 16.12684778869152, Val Accuracy 0.934850863422292, Precision 0.9420661258603736, Recall 0.8182974693906448, F1 0.8648123786255716, Cohen Kappa 0.7312605153072376, Accuracy 0.934850863422292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 54.8611516058445, Val Loss 14.28318503126502, Val Accuracy 0.9419152276295133, Precision 0.9230937215650592, Recall 0.86006203731974, F1 0.8875095456281024, Cohen Kappa 0.775406063435947, Accuracy 0.9419152276295133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 47.98806459642947, Val Loss 12.849208252504468, Val Accuracy 0.9489795918367347, Precision 0.9370032290114256, Recall 0.873684140659224, F1 0.9013991767587346, Cohen Kappa 0.8031192791421983, Accuracy 0.9489795918367347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 44.39594913460314, Val Loss 14.969655133783817, Val Accuracy 0.9497645211930926, Precision 0.9630661016460549, Recall 0.8553937511338847, F1 0.8983645791783008, Cohen Kappa 0.7975779663457481, Accuracy 0.9497645211930926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 38.82994038332254, Val Loss 15.437961286865175, Val Accuracy 0.9474097331240189, Precision 0.9182369224894997, Recall 0.8877509480382135, F1 0.9020435237156395, Cohen Kappa 0.8041681770419892, Accuracy 0.9474097331240189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 34.72752769244835, Val Loss 14.214955976931378, Val Accuracy 0.9513343799058085, Precision 0.9429663330300273, Recall 0.8769740655684026, F1 0.9057512409316533, Cohen Kappa 0.8118267017976853, Accuracy 0.9513343799058085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 31.27614844031632, Val Loss 13.565820374991745, Val Accuracy 0.9505494505494505, Precision 0.9309327561669829, Recall 0.8858836335638713, F1 0.9063946511229213, Cohen Kappa 0.8129483325176536, Accuracy 0.9505494505494505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 28.061433617956936, Val Loss 13.442584184929729, Val Accuracy 0.9489795918367347, Precision 0.9197588126159555, Recall 0.8924457837189648, F1 0.9053438814152353, Cohen Kappa 0.8107502799552071, Accuracy 0.9489795918367347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 24.622859959490597, Val Loss 14.490997820859775, Val Accuracy 0.9489795918367347, Precision 0.9228377233518177, Recall 0.8886934551070166, F1 0.9045874307967947, Cohen Kappa 0.8092717733279899, Accuracy 0.9489795918367347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 21.762996871722862, Val Loss 17.42864704108797, Val Accuracy 0.945839874411303, Precision 0.926691042047532, Recall 0.8717991265216178, F1 0.8962044597155558, Cohen Kappa 0.7926755062476121, Accuracy 0.945839874411303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 18.406319587142207, Val Loss 13.654720789636485, Val Accuracy 0.9544740973312402, Precision 0.9517359415770215, Recall 0.8807352440119827, F1 0.9114569603082926, Cohen Kappa 0.8232527878373271, Accuracy 0.9544740973312402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 18.443142672185786, Val Loss 15.422618018230423, Val Accuracy 0.9544740973312402, Precision 0.9475044935082325, Recall 0.8844875726239309, F1 0.91220265678097, Cohen Kappa 0.8246752938608355, Accuracy 0.9544740973312402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 16.454145188443363, Val Loss 15.143042417126708, Val Accuracy 0.9544740973312402, Precision 0.9495905368516833, Recall 0.8826114083179568, F1 0.911831806032837, Cohen Kappa 0.8239669145849314, Accuracy 0.9544740973312402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 15.672327698906884, Val Loss 15.429984479269478, Val Accuracy 0.9529042386185244, Precision 0.9383931120494058, Recall 0.8872973941670759, F1 0.9103019037963276, Cohen Kappa 0.820793068039555, Accuracy 0.9529042386185244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 14.037741501117125, Val Loss 15.578921579406597, Val Accuracy 0.956828885400314, Precision 0.9435236796331435, Recall 0.8971583190629798, F1 0.9182810446311218, Cohen Kappa 0.8367009252138246, Accuracy 0.956828885400314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 11.76916406134842, Val Loss 18.42581351532135, Val Accuracy 0.9481946624803768, Precision 0.9106665036741236, Recall 0.9013553517144337, F1 0.9059210526315791, Cohen Kappa 0.8118496838204691, Accuracy 0.9481946624803768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 10.307011318102013, Val Loss 17.070664786850102, Val Accuracy 0.9529042386185244, Precision 0.9383931120494058, Recall 0.8872973941670759, F1 0.9103019037963276, Cohen Kappa 0.820793068039555, Accuracy 0.9529042386185244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 11.785572231572587, Val Loss 16.096017508709338, Val Accuracy 0.9466248037676609, Precision 0.915971578622181, Recall 0.887279694503812, F1 0.9007788161993769, Cohen Kappa 0.8016312436744657, Accuracy 0.9466248037676609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 10.29227663751226, Val Loss 18.025155917101074, Val Accuracy 0.945054945054945, Precision 0.9087553843086524, Recall 0.8900895160469572, F1 0.8990578125495202, Cohen Kappa 0.7981485253842149, Accuracy 0.945054945054945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 9.820572069147602, Val Loss 16.966128055151785, Val Accuracy 0.9607535321821036, Precision 0.956382403340191, Recall 0.8995145867349874, F1 0.9249419095155278, Cohen Kappa 0.8500647287277863, Accuracy 0.9607535321821036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 9.314075839705765, Val Loss 17.335797121515498, Val Accuracy 0.9607535321821036, Precision 0.9651131656455245, Recall 0.892009929511091, F1 0.9236697933692177, Cohen Kappa 0.8476317136528682, Accuracy 0.9607535321821036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 7.074975162075134, Val Loss 17.56719311408233, Val Accuracy 0.9544740973312402, Precision 0.9295231631388186, Recall 0.9032492156836717, F1 0.9157042141048165, Cohen Kappa 0.8314576500052462, Accuracy 0.9544740973312402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 6.396395941294031, Val Loss 19.710345028026495, Val Accuracy 0.9332810047095761, Precision 0.8720961016612615, Recall 0.8980300274787272, F1 0.8842851571056864, Cohen Kappa 0.7686595541959163, Accuracy 0.9332810047095761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 7.358453562279465, Val Loss 24.694209061999572, Val Accuracy 0.945839874411303, Precision 0.9551948051948052, Recall 0.8474089905439549, F1 0.8901761184967524, Cohen Kappa 0.7813033332172338, Accuracy 0.945839874411303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 6.692351615114603, Val Loss 21.71553630006383, Val Accuracy 0.9395604395604396, Precision 0.8860714285714286, Recall 0.9018000557539393, F1 0.8936615115115658, Cohen Kappa 0.7873509149949925, Accuracy 0.9395604395604396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 5.808263787912438, Val Loss 20.167968931113137, Val Accuracy 0.9529042386185244, Precision 0.9347662457138077, Recall 0.8910497227790242, F1 0.9110335195530725, Cohen Kappa 0.8222069228586447, Accuracy 0.9529042386185244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 7.216050586124766, Val Loss 19.442648761731107, Val Accuracy 0.9536891679748822, Precision 0.9390556441962505, Recall 0.8896448120074516, F1 0.9119781438426506, Cohen Kappa 0.8241294917570204, Accuracy 0.9536891679748822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 6.07691810556571, Val Loss 16.836617182183545, Val Accuracy 0.9583987441130298, Precision 0.9429986405127209, Recall 0.9037293190497051, F1 0.9218888638243476, Cohen Kappa 0.8438732710263086, Accuracy 0.9583987441130298\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['s'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_S_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpAPWBipQL0-",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 295.41145169734955, Val Loss 65.10073530673981, Val Accuracy 0.6420722135007849, Precision 0.8116809116809117, Recall 0.42891265597147954, F1 0.40288412420222985, Cohen Kappa 0.19236608111038822, Accuracy 0.6420722135007849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 233.26579973101616, Val Loss 52.687441393733025, Val Accuracy 0.7409733124018838, Precision 0.7551023774561818, Recall 0.6293707877935674, F1 0.6658477198978174, Cohen Kappa 0.49375776666762994, Accuracy 0.7409733124018838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 195.97863084077835, Val Loss 50.35385200381279, Val Accuracy 0.7511773940345369, Precision 0.8206095648892887, Recall 0.6187200573818807, F1 0.6672141605250338, Cohen Kappa 0.49529925905375316, Accuracy 0.7511773940345369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 177.21515980362892, Val Loss 46.08849556744099, Val Accuracy 0.7668759811616954, Precision 0.7746118669366723, Recall 0.6760296425705379, F1 0.7104258227536128, Cohen Kappa 0.5557999997652083, Accuracy 0.7668759811616954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 159.605928465724, Val Loss 45.79635575413704, Val Accuracy 0.7700156985871272, Precision 0.7527918015024646, Recall 0.7078396574131283, F1 0.7266247278030386, Cohen Kappa 0.5795677888118866, Accuracy 0.7700156985871272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 149.09803292900324, Val Loss 50.00240220874548, Val Accuracy 0.7723704866562009, Precision 0.7992954361465291, Recall 0.6749952063404138, F1 0.7119575429532249, Cohen Kappa 0.5618207931724407, Accuracy 0.7723704866562009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 136.46380247175694, Val Loss 48.38234497606754, Val Accuracy 0.7739403453689168, Precision 0.8022082931877312, Recall 0.6781333844657023, F1 0.7178786650821302, Cohen Kappa 0.5640205755595717, Accuracy 0.7739403453689168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 126.90148751065135, Val Loss 47.414471447467804, Val Accuracy 0.7653061224489796, Precision 0.7606188159973394, Recall 0.6974298882899772, F1 0.7199834957221193, Cohen Kappa 0.5667721324638281, Accuracy 0.7653061224489796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 117.7955002039671, Val Loss 47.56589266657829, Val Accuracy 0.7794348508634223, Precision 0.7713942803863372, Recall 0.7124655744224527, F1 0.7348569546268869, Cohen Kappa 0.5936494748001697, Accuracy 0.7794348508634223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 107.37706707417965, Val Loss 49.03364862501621, Val Accuracy 0.7770800627943485, Precision 0.7869335851341929, Recall 0.6909944535583158, F1 0.7247406636405421, Cohen Kappa 0.5773231886038848, Accuracy 0.7770800627943485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 101.64276596158743, Val Loss 55.57043185085058, Val Accuracy 0.7849293563579278, Precision 0.7848739491146463, Recall 0.7112704263161259, F1 0.7368772893772894, Cohen Kappa 0.6004578282914992, Accuracy 0.7849293563579278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 91.71453055553138, Val Loss 53.20378457009792, Val Accuracy 0.7857142857142857, Precision 0.7680525561666348, Recall 0.7272961629418156, F1 0.7448275191916124, Cohen Kappa 0.6098867017893737, Accuracy 0.7857142857142857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 82.65133855026215, Val Loss 56.91611439734697, Val Accuracy 0.782574568288854, Precision 0.7949686745124303, Recall 0.6965107129414605, F1 0.7276540163504971, Cohen Kappa 0.5894343608972696, Accuracy 0.782574568288854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 76.53538224566728, Val Loss 56.37416396290064, Val Accuracy 0.7880690737833596, Precision 0.7811865676647639, Recall 0.7145596579812658, F1 0.7407912824023734, Cohen Kappa 0.6062319703283117, Accuracy 0.7880690737833596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 70.47478351462632, Val Loss 57.19734523445368, Val Accuracy 0.7794348508634223, Precision 0.7564325188260889, Recall 0.7305321317226637, F1 0.739754671005629, Cohen Kappa 0.6048461247228083, Accuracy 0.7794348508634223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 62.663162456825376, Val Loss 59.852864153683186, Val Accuracy 0.7755102040816326, Precision 0.7573539746649445, Recall 0.7291863561795598, F1 0.7371397088173852, Cohen Kappa 0.5996884204697643, Accuracy 0.7755102040816326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 57.16452729096636, Val Loss 65.09232018329203, Val Accuracy 0.7841444270015698, Precision 0.7869308025477371, Recall 0.7162272833798496, F1 0.7367627005906133, Cohen Kappa 0.602223958073145, Accuracy 0.7841444270015698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 55.73195459600538, Val Loss 63.675119280815125, Val Accuracy 0.7951334379905809, Precision 0.7928425950885333, Recall 0.7202192300317446, F1 0.7484278543136367, Cohen Kappa 0.618074810538624, Accuracy 0.7951334379905809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 48.32309991680086, Val Loss 65.3193495310843, Val Accuracy 0.7888540031397174, Precision 0.7775367409920553, Recall 0.7205314215508731, F1 0.7436709529469803, Cohen Kappa 0.6105494145843111, Accuracy 0.7888540031397174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 42.31380594451912, Val Loss 72.81242588162422, Val Accuracy 0.7770800627943485, Precision 0.7695530700597489, Recall 0.7083470751574806, F1 0.729530937096412, Cohen Kappa 0.5896972989032996, Accuracy 0.7770800627943485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 43.00100221252069, Val Loss 65.39563447609544, Val Accuracy 0.771585557299843, Precision 0.7416083479838939, Recall 0.7262627919693774, F1 0.7314445655493763, Cohen Kappa 0.5948775892892266, Accuracy 0.771585557299843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 36.608422473073006, Val Loss 75.32244351878762, Val Accuracy 0.7951334379905809, Precision 0.7856000005238776, Recall 0.7293460027980769, F1 0.7523271114312262, Cohen Kappa 0.6228499096579251, Accuracy 0.7951334379905809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 46.29313408466987, Val Loss 69.14906881377101, Val Accuracy 0.792778649921507, Precision 0.7741539192211553, Recall 0.7434284963532679, F1 0.7557491216743565, Cohen Kappa 0.6276717751612918, Accuracy 0.792778649921507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 31.044659147737548, Val Loss 75.80271407216787, Val Accuracy 0.7755102040816326, Precision 0.7407414050504703, Recall 0.744284324378067, F1 0.7420547514280327, Cohen Kappa 0.6078776595172243, Accuracy 0.7755102040816326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 28.717798846773803, Val Loss 67.47978933155537, Val Accuracy 0.7967032967032966, Precision 0.7773017427710066, Recall 0.7438918834466057, F1 0.7584093096908192, Cohen Kappa 0.6325946611616995, Accuracy 0.7967032967032966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 32.354475614614785, Val Loss 77.06024838518351, Val Accuracy 0.7817896389324961, Precision 0.762706815173155, Recall 0.7334020069454801, F1 0.7432108098225355, Cohen Kappa 0.6091269775577886, Accuracy 0.7817896389324961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 25.043858746998012, Val Loss 78.86814760416746, Val Accuracy 0.7904238618524333, Precision 0.773073422739059, Recall 0.7281603709937433, F1 0.7467553658475076, Cohen Kappa 0.6172598633577273, Accuracy 0.7904238618524333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 23.298749006702565, Val Loss 87.44531255960464, Val Accuracy 0.7912087912087912, Precision 0.7857395929172114, Recall 0.7168768775166713, F1 0.7437631147356835, Cohen Kappa 0.6115809139569268, Accuracy 0.7912087912087912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 23.955150995985605, Val Loss 81.00945076346397, Val Accuracy 0.7967032967032966, Precision 0.7850470795338409, Recall 0.7272424029372705, F1 0.7506850239675268, Cohen Kappa 0.624945298886882, Accuracy 0.7967032967032966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 27.23011586663779, Val Loss 83.1487877741456, Val Accuracy 0.7974882260596546, Precision 0.7779147302222604, Recall 0.7394619738514748, F1 0.7560662050067157, Cohen Kappa 0.6322376650607209, Accuracy 0.7974882260596546\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['c'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_C_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-C')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD3qEBlMQMGg",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 312.2672570347786, Val Loss 70.74249792098999, Val Accuracy 0.5777080062794349, Precision 0.6987626385216746, Recall 0.44040351556397767, F1 0.40409419649146955, Cohen Kappa 0.1659513573435516, Accuracy 0.5777080062794349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 239.84650120139122, Val Loss 54.305843308568, Val Accuracy 0.7378335949764521, Precision 0.7639500119527267, Recall 0.683466627272789, F1 0.70789354657431, Cohen Kappa 0.5419481166755833, Accuracy 0.7378335949764521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 196.18412137031555, Val Loss 47.281600296497345, Val Accuracy 0.7668759811616954, Precision 0.783916100545314, Recall 0.7384513961054399, F1 0.7550792287634392, Cohen Kappa 0.6094090006317536, Accuracy 0.7668759811616954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 172.72525110840797, Val Loss 46.31010055541992, Val Accuracy 0.750392464678179, Precision 0.7435845866009231, Recall 0.7487958548131847, F1 0.7430980063784086, Cohen Kappa 0.5965736864536739, Accuracy 0.750392464678179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 154.8957176655531, Val Loss 43.04283928871155, Val Accuracy 0.7974882260596546, Precision 0.8217347905695765, Recall 0.7553525582248303, F1 0.7803202271716723, Cohen Kappa 0.6540058716222119, Accuracy 0.7974882260596546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 141.31132075935602, Val Loss 42.57197703421116, Val Accuracy 0.793563579277865, Precision 0.7899765457632381, Recall 0.7733453722381837, F1 0.7804044889224538, Cohen Kappa 0.655474929899551, Accuracy 0.793563579277865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 131.87776032090187, Val Loss 42.11679530143738, Val Accuracy 0.8037676609105181, Precision 0.8099099822104517, Recall 0.7834804660703248, F1 0.7952820032555131, Cohen Kappa 0.6717466069317635, Accuracy 0.8037676609105181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 119.13777748122811, Val Loss 43.397055834531784, Val Accuracy 0.8108320251177394, Precision 0.8153446296087425, Recall 0.7938561681982735, F1 0.8030737242989017, Cohen Kappa 0.6860368329021505, Accuracy 0.8108320251177394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 108.52421648427844, Val Loss 43.8794309347868, Val Accuracy 0.8029827315541601, Precision 0.8197559628973942, Recall 0.7768870856001794, F1 0.7920348722145265, Cohen Kappa 0.6710539986441961, Accuracy 0.8029827315541601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 97.91744743660092, Val Loss 45.03292355686426, Val Accuracy 0.804552590266876, Precision 0.8075619158229359, Recall 0.7805298306742466, F1 0.7916043161486493, Cohen Kappa 0.6715100806660385, Accuracy 0.804552590266876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 89.48313337936997, Val Loss 45.10222774744034, Val Accuracy 0.7998430141287284, Precision 0.790435262212406, Recall 0.7932189892074359, F1 0.7917120551051902, Cohen Kappa 0.672116422034923, Accuracy 0.7998430141287284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 82.21301101893187, Val Loss 46.001490734517574, Val Accuracy 0.7990580847723705, Precision 0.786891961229414, Recall 0.7902065461757375, F1 0.7883587774877587, Cohen Kappa 0.6710825682151904, Accuracy 0.7990580847723705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 74.87006101757288, Val Loss 50.28335529565811, Val Accuracy 0.7998430141287284, Precision 0.7852868471944731, Recall 0.7987647811941779, F1 0.7909700905056792, Cohen Kappa 0.6756518761313047, Accuracy 0.7998430141287284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 73.98642206192017, Val Loss 47.77083179354668, Val Accuracy 0.8108320251177394, Precision 0.8306842571037804, Recall 0.7834245165759928, F1 0.7993842205303977, Cohen Kappa 0.6840459862847923, Accuracy 0.8108320251177394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 62.52719427086413, Val Loss 48.715648140758276, Val Accuracy 0.8092621664050236, Precision 0.8093637825774386, Recall 0.7935743831122521, F1 0.8004876576225851, Cohen Kappa 0.6841593007027251, Accuracy 0.8092621664050236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 54.521184724755585, Val Loss 50.03361587226391, Val Accuracy 0.8076923076923077, Precision 0.8196950682025309, Recall 0.7768751995164062, F1 0.7947398939095262, Cohen Kappa 0.6748168220540726, Accuracy 0.8076923076923077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 57.590999734122306, Val Loss 49.044292494654655, Val Accuracy 0.8186813186813187, Precision 0.8141536238826351, Recall 0.8055524991340138, F1 0.8092181620842548, Cohen Kappa 0.7012046480224583, Accuracy 0.8186813186813187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 49.877284062094986, Val Loss 53.21846433356404, Val Accuracy 0.8084772370486656, Precision 0.8041074897596235, Recall 0.8008645427932976, F1 0.7993272277193806, Cohen Kappa 0.6881335527107829, Accuracy 0.8084772370486656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 44.07616909593344, Val Loss 54.08578570559621, Val Accuracy 0.8155416012558869, Precision 0.8405954605757432, Recall 0.7741341836977268, F1 0.799525668750371, Cohen Kappa 0.6847268541870914, Accuracy 0.8155416012558869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 41.425229073967785, Val Loss 51.82762245833874, Val Accuracy 0.8100470957613815, Precision 0.8065810444933116, Recall 0.794659837262533, F1 0.800271937445347, Cohen Kappa 0.6847901654629773, Accuracy 0.8100470957613815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 36.90915163885802, Val Loss 55.11288552172482, Val Accuracy 0.8076923076923077, Precision 0.7990423894422349, Recall 0.7958037879250973, F1 0.7973757348867615, Cohen Kappa 0.68325438286714, Accuracy 0.8076923076923077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 32.99354202207178, Val Loss 60.9539192430675, Val Accuracy 0.8155416012558869, Precision 0.8256492010113785, Recall 0.801825277964559, F1 0.8083429775189547, Cohen Kappa 0.6967926190741435, Accuracy 0.8155416012558869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 29.47533327760175, Val Loss 62.99900307599455, Val Accuracy 0.8139717425431711, Precision 0.8193665181407028, Recall 0.8009896862753089, F1 0.8061915884597782, Cohen Kappa 0.694399995141784, Accuracy 0.8139717425431711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 32.0409739818424, Val Loss 60.85997924208641, Val Accuracy 0.8092621664050236, Precision 0.8089025986152744, Recall 0.8032397219335602, F1 0.8044619266753492, Cohen Kappa 0.6877186399234987, Accuracy 0.8092621664050236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 29.064176907530054, Val Loss 77.28436710685492, Val Accuracy 0.7841444270015698, Precision 0.767833614200227, Recall 0.7783840529508051, F1 0.770735593702704, Cohen Kappa 0.6470647492885385, Accuracy 0.7841444270015698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 32.07269841758534, Val Loss 66.52790614031255, Val Accuracy 0.8163265306122449, Precision 0.8276154052473119, Recall 0.7866450509743191, F1 0.8038469553189133, Cohen Kappa 0.6903424435562278, Accuracy 0.8163265306122449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 21.766161640523933, Val Loss 73.52043160842732, Val Accuracy 0.8053375196232339, Precision 0.8042546183662517, Recall 0.7841795375973809, F1 0.7918852391354313, Cohen Kappa 0.6741072948137024, Accuracy 0.8053375196232339\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 18.589789773104712, Val Loss 72.06382697308436, Val Accuracy 0.8116169544740973, Precision 0.8264235712536291, Recall 0.7809215620351692, F1 0.7995335063787005, Cohen Kappa 0.6804855415945628, Accuracy 0.8116169544740973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 21.893829099601135, Val Loss 71.12287958152592, Val Accuracy 0.8014128728414442, Precision 0.7940327902497554, Recall 0.8010418152427139, F1 0.7953343349997706, Cohen Kappa 0.6781569025290368, Accuracy 0.8014128728414442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 22.62504480103962, Val Loss 71.00515733519569, Val Accuracy 0.8037676609105181, Precision 0.802386933654394, Recall 0.7845880792767828, F1 0.7925535843426937, Cohen Kappa 0.6741754644406253, Accuracy 0.8037676609105181\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['i'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_I_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-I')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwlDcXQAQMW-",
        "outputId": "759c1caa-06f7-443c-b399-d40621de7e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "Let's use 2 GPUs!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss 233.07294330000877, Val Loss 47.92720142006874, Val Accuracy 0.749607535321821, Precision 0.8296536855381974, Recall 0.5176824310682578, F1 0.5028540062092876, Cohen Kappa 0.505426958457208, Accuracy 0.749607535321821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss 173.8499368429184, Val Loss 43.324701592326164, Val Accuracy 0.7857142857142857, Precision 0.8508546918693888, Recall 0.5280938115583785, F1 0.5226730704275376, Cohen Kappa 0.5578447151432865, Accuracy 0.7857142857142857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss 156.40486924350262, Val Loss 40.89828485995531, Val Accuracy 0.8061224489795918, Precision 0.8651813389584398, Recall 0.5537784907863648, F1 0.5407571667058781, Cohen Kappa 0.6120130250427531, Accuracy 0.8061224489795918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss 140.39701183140278, Val Loss 38.9397366642952, Val Accuracy 0.814756671899529, Precision 0.8726385399520633, Recall 0.5637014967723629, F1 0.5475480420711557, Cohen Kappa 0.6331052627083174, Accuracy 0.814756671899529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss 128.58084670454264, Val Loss 35.65757132321596, Val Accuracy 0.8430141287284144, Precision 0.8898169769509202, Recall 0.5745939761687794, F1 0.5651373324544057, Cohen Kappa 0.6807525619356318, Accuracy 0.8430141287284144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss 116.35969979315996, Val Loss 34.332725398242474, Val Accuracy 0.8430141287284144, Precision 0.8900625345095472, Recall 0.5733161993004513, F1 0.5648032386735726, Cohen Kappa 0.6796246025169839, Accuracy 0.8430141287284144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss 107.78581376373768, Val Loss 33.628235813230276, Val Accuracy 0.8492935635792779, Precision 0.8972980424593328, Recall 0.5729824312501478, F1 0.568190289814556, Cohen Kappa 0.6879187951967094, Accuracy 0.8492935635792779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss 98.39577378705144, Val Loss 35.53775632753968, Val Accuracy 0.847723704866562, Precision 0.8994101012752128, Recall 0.5677630992591622, F1 0.5656462849193601, Cohen Kappa 0.6811240057542076, Accuracy 0.847723704866562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss 91.49350661970675, Val Loss 33.29948423616588, Val Accuracy 0.8500784929356358, Precision 0.8951539201539201, Recall 0.5777643179217983, F1 0.5696341555004794, Cohen Kappa 0.6935000352683924, Accuracy 0.8500784929356358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss 84.3936178162694, Val Loss 36.45640008896589, Val Accuracy 0.8540031397174255, Precision 0.9011987950682109, Recall 0.5753515477924926, F1 0.5711737943585078, Cohen Kappa 0.6969107055379118, Accuracy 0.8540031397174255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss 77.37389226444066, Val Loss 39.51341077964753, Val Accuracy 0.8500784929356358, Precision 0.9036690085870415, Recall 0.5672865476015082, F1 0.5666022900847243, Cohen Kappa 0.6841081749118534, Accuracy 0.8500784929356358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss 70.20743157435209, Val Loss 36.562281250953674, Val Accuracy 0.8547880690737834, Precision 0.9072557477265745, Recall 0.5709334410121811, F1 0.5702671966207419, Cohen Kappa 0.6941430516137655, Accuracy 0.8547880690737834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss 63.678892123512924, Val Loss 42.138379245996475, Val Accuracy 0.8383045525902669, Precision 0.8864138129784861, Recall 0.5752915241104217, F1 0.5625451341183955, Cohen Kappa 0.6752907232107175, Accuracy 0.8383045525902669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss 58.91469216905534, Val Loss 42.858775585889816, Val Accuracy 0.8390894819466248, Precision 0.9020281555714625, Recall 0.5550289841628424, F1 0.5568389046412724, Cohen Kappa 0.6565690037647622, Accuracy 0.8390894819466248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss 52.99806383624673, Val Loss 38.76933620031923, Val Accuracy 0.847723704866562, Precision 0.8950047146768458, Recall 0.5733853174798057, F1 0.5674115867041912, Cohen Kappa 0.686130784342859, Accuracy 0.847723704866562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss 52.25881375139579, Val Loss 48.360779479146004, Val Accuracy 0.8437990580847724, Precision 0.9005834086118639, Recall 0.5614869866838371, F1 0.5614985442783721, Cohen Kappa 0.6696721033800739, Accuracy 0.8437990580847724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss 42.488565982319415, Val Loss 49.89406825369224, Val Accuracy 0.8547880690737834, Precision 0.9019107847137223, Recall 0.5853375698435389, F1 0.5918588147441214, Cohen Kappa 0.6982801108614808, Accuracy 0.8547880690737834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss 43.298581291455775, Val Loss 48.845571268349886, Val Accuracy 0.8563579277864992, Precision 0.9039672427676292, Recall 0.5748749961348386, F1 0.5718870033202405, Cohen Kappa 0.700291170402175, Accuracy 0.8563579277864992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Loss 36.08191264839843, Val Loss 49.163598597981036, Val Accuracy 0.8555729984301413, Precision 0.6999221910986617, Recall 0.6022235869835565, F1 0.610538045949373, Cohen Kappa 0.7060861344564155, Accuracy 0.8555729984301413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Loss 36.55762976082042, Val Loss 48.350623393431306, Val Accuracy 0.8500784929356358, Precision 0.6477172312223859, Recall 0.5963503606378892, F1 0.6037544413570095, Cohen Kappa 0.6942371824996921, Accuracy 0.8500784929356358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Loss 40.212136460933834, Val Loss 43.682731330394745, Val Accuracy 0.8579277864992151, Precision 0.6801510474141744, Recall 0.5916873478468602, F1 0.5945243610150805, Cohen Kappa 0.7085987100165797, Accuracy 0.8579277864992151\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Loss 31.273144792299718, Val Loss 49.476320227608085, Val Accuracy 0.8516483516483516, Precision 0.6293505023555958, Recall 0.5946696975399033, F1 0.6024639443244094, Cohen Kappa 0.6955906113067861, Accuracy 0.8516483516483516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Loss 27.776803236454725, Val Loss 51.32112854276784, Val Accuracy 0.8492935635792779, Precision 0.6162714354796711, Recall 0.5892684756032482, F1 0.5976393579891387, Cohen Kappa 0.6885910414926772, Accuracy 0.8492935635792779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Loss 23.1197482093703, Val Loss 59.492810553638265, Val Accuracy 0.8390894819466248, Precision 0.6157587038073611, Recall 0.572755332851853, F1 0.5782817395070496, Cohen Kappa 0.6659029651319719, Accuracy 0.8390894819466248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Loss 27.877105187508278, Val Loss 59.058906319784, Val Accuracy 0.8485086342229199, Precision 0.6569239283942135, Recall 0.5958240532789681, F1 0.6124435618686505, Cohen Kappa 0.6832519613021244, Accuracy 0.8485086342229199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Loss 24.93421693169512, Val Loss 55.325403708731756, Val Accuracy 0.858712715855573, Precision 0.649983164983165, Recall 0.5965622624245941, F1 0.6077972697548691, Cohen Kappa 0.7069419187304394, Accuracy 0.858712715855573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Loss 25.444507300155237, Val Loss 52.46228240057826, Val Accuracy 0.8461538461538461, Precision 0.6176355338764415, Recall 0.6052450430149185, F1 0.6095131672979556, Cohen Kappa 0.6893282156876163, Accuracy 0.8461538461538461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Loss 17.62007561873179, Val Loss 56.999499479308724, Val Accuracy 0.8492935635792779, Precision 0.6247468218056454, Recall 0.6054614920502652, F1 0.6119659009003664, Cohen Kappa 0.6939346998631135, Accuracy 0.8492935635792779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Loss 22.77806202962529, Val Loss 56.33443807903677, Val Accuracy 0.8516483516483516, Precision 0.6221944430180412, Recall 0.5903252561875879, F1 0.5998220315548911, Cohen Kappa 0.6924002606062929, Accuracy 0.8516483516483516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/leili/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Loss 20.545597523334436, Val Loss 58.374450035451446, Val Accuracy 0.8540031397174255, Precision 0.637857490684861, Recall 0.5831112369085445, F1 0.5893057177105077, Cohen Kappa 0.6962564651443001, Accuracy 0.8540031397174255\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using:\", device)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
        "        nn.init.xavier_uniform_(self.attention_weights)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attention_scores = torch.matmul(lstm_output, self.attention_weights).squeeze(2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        context_vector = torch.sum(lstm_output * attention_weights.unsqueeze(2), dim=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class LSTMNetWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
        "        super(LSTMNetWithAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        context_vector, attention_weights = self.attention(out)\n",
        "        out = self.dropout(context_vector)\n",
        "        out = self.fc(out)\n",
        "        return out, attention_weights\n",
        "\n",
        "# Load data\n",
        "token_embeddings_np = np.array(df['layer_12_output'].tolist())\n",
        "labels = np.array(df['a'])\n",
        "input_dim = token_embeddings_np.shape[2]  # Input dim is 768\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(token_embeddings_np, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 16  # Adjust batch size suitable for your GPU configuration\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "hidden_dim = 128\n",
        "output_dim = len(np.unique(labels_encoded))\n",
        "n_layers = 2\n",
        "\n",
        "# Model instantiation and moving to GPU if available\n",
        "model = LSTMNetWithAttention(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "\n",
        "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
        "\n",
        "# Custom metrics for logging\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
        "    cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    return precision, recall, f1, cohen_kappa, accuracy\n",
        "\n",
        "# Create an empty DataFrame to store evaluation metrics\n",
        "evaluation_metrics = pd.DataFrame(columns=['Epoch', 'Precision', 'Recall', 'F1', 'Cohen Kappa', 'Accuracy'])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Compute validation metrics\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attention_weights = model(inputs)\n",
        "            val_loss = F.cross_entropy(outputs, labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.tolist())\n",
        "            all_labels.extend(labels.tolist())\n",
        "\n",
        "    val_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, f1, cohen_kappa, accuracy = compute_metrics(all_labels, all_predictions)\n",
        "\n",
        "    new_metrics = pd.DataFrame([{'Epoch': epoch+1, 'Precision': precision, 'Recall': recall, 'F1': f1, 'Cohen Kappa': cohen_kappa, 'Accuracy': accuracy, 'Loss': total_loss, 'Val_Loss': total_val_loss, 'Val_Accuracy': val_accuracy}])\n",
        "    evaluation_metrics = pd.concat([evaluation_metrics, new_metrics], ignore_index=True)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss {total_loss}, Val Loss {total_val_loss}, Val Accuracy {val_accuracy}, Precision {precision}, Recall {recall}, F1 {f1}, Cohen Kappa {cohen_kappa}, Accuracy {accuracy}')\n",
        "    evaluation_metrics.to_csv('/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/eva_A_B_LSTM_ATT.csv', index=False)\n",
        "    scheduler.step()\n",
        "    #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model.state_dict(), '/home/leili/Vul/LSTM/BERT_LSTM_ATT_TORCH/original/model-A')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027c3ccd1f534c9198c345413d928c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72266fdc9c5a43d4ae7408b3e73715e4",
            "placeholder": "",
            "style": "IPY_MODEL_9ebe3a01e76444ffbc5a57aed228ac54",
            "value": "232k/232k[00:00&lt;00:00,4.83MB/s]"
          }
        },
        "07571df3674a45f581cc2b555439c60c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0da5223538714fa8bd96a05bef5f4714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_714dc8f6b26b4781bff8db0aa57f4b38",
              "IPY_MODEL_d6ed21b8f9644662abdf1d526e49a3b5",
              "IPY_MODEL_1da18aec495444989fe705b6e34e7f7c"
            ],
            "layout": "IPY_MODEL_2e47a725420f4bc49cbd65d8c25fa834"
          }
        },
        "12fac696c631479db823806b69fb07eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15ff7af9a7d74f01bb7f565f821322e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1919ef6fa05c4feda2c8764a070af4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1da18aec495444989fe705b6e34e7f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15ff7af9a7d74f01bb7f565f821322e3",
            "placeholder": "",
            "style": "IPY_MODEL_5b9b2447d15b4bc0b4386c9e678324a7",
            "value": "440M/440M[00:03&lt;00:00,108MB/s]"
          }
        },
        "1f86d7fda26147c68ea723b810388fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff88f9f79cb415aaa45deb82d14ef4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28b6dec78c5449e98e2f221675edac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad0409c1db3441685ed851cbede81da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e47a725420f4bc49cbd65d8c25fa834": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3055afcd834d4153a5217f5ebad676a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4564813240954db5b8ab9d52c2203e74",
            "placeholder": "",
            "style": "IPY_MODEL_627db4cc35ad4c9ab1f51fb924e276e3",
            "value": "570/570[00:00&lt;00:00,18.9kB/s]"
          }
        },
        "31f03477178d4e52a241c364f3ca9645": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55160331a9e44a4aa05a1447dd430709",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1919ef6fa05c4feda2c8764a070af4a2",
            "value": 466062
          }
        },
        "3abb2234b9474682bf3764d555b82660": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4564813240954db5b8ab9d52c2203e74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490ea7edeb3644c98f5d80e4c97c8eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b8ba04d004b4624950c03311629d1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cac51aa9c744bee97a09dfe15a630e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12fac696c631479db823806b69fb07eb",
            "placeholder": "",
            "style": "IPY_MODEL_bbd250209a784cf1996ee289d0692aed",
            "value": "tokenizer_config.json:100%"
          }
        },
        "52ccf50e0cf6427c8ed3f251dd96405b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f86d7fda26147c68ea723b810388fd8",
            "placeholder": "",
            "style": "IPY_MODEL_83bc0befebbf4842982e8b4a38e7f07d",
            "value": "48.0/48.0[00:00&lt;00:00,557B/s]"
          }
        },
        "55160331a9e44a4aa05a1447dd430709": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9b2447d15b4bc0b4386c9e678324a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d61134be7404122b424cf540c0bdd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f84b04465d104da783ed7205e76e2bc1",
              "IPY_MODEL_31f03477178d4e52a241c364f3ca9645",
              "IPY_MODEL_5dc0d027289449abb25e9bffb8089927"
            ],
            "layout": "IPY_MODEL_d851b8c85fe3472b8e123302ac79213d"
          }
        },
        "5dc0d027289449abb25e9bffb8089927": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea9deb568a043819b2dd05cac85df34",
            "placeholder": "",
            "style": "IPY_MODEL_2ad0409c1db3441685ed851cbede81da",
            "value": "466k/466k[00:00&lt;00:00,7.73MB/s]"
          }
        },
        "5e33156da7be42f89533f6b650f702ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "627db4cc35ad4c9ab1f51fb924e276e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d39cd82cbd24b78b7bd3181e227d30b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f6d391d4e2a434d9d05ff73ccca43f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "714dc8f6b26b4781bff8db0aa57f4b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb5f890586d544d2958336f144fc8394",
            "placeholder": "",
            "style": "IPY_MODEL_28b6dec78c5449e98e2f221675edac28",
            "value": "model.safetensors:100%"
          }
        },
        "72266fdc9c5a43d4ae7408b3e73715e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7404d7c9c6fc4c728b56f7172d080b48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7826ddf934b6446f8b3fbebfb0aa3d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5fecc3c4e154dbfb7b9ac1d776fdd7e",
              "IPY_MODEL_83387cb3fb364845aafc2d59afbe5beb",
              "IPY_MODEL_3055afcd834d4153a5217f5ebad676a9"
            ],
            "layout": "IPY_MODEL_7404d7c9c6fc4c728b56f7172d080b48"
          }
        },
        "78ee13f761de4fe986de320730542b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cbcbb0bc28a41d68804f74901aaf677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dea267805064184a394d924cef06e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83387cb3fb364845aafc2d59afbe5beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dea267805064184a394d924cef06e4d",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97693fb687c8418899da26c59eee2b87",
            "value": 570
          }
        },
        "83bc0befebbf4842982e8b4a38e7f07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89d9191237164b988943d82af185189b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dbdd7e977b4e66b674592f1e48c53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4cac51aa9c744bee97a09dfe15a630e3",
              "IPY_MODEL_c8fb9f2f59f54f0691a6e0a28869e556",
              "IPY_MODEL_52ccf50e0cf6427c8ed3f251dd96405b"
            ],
            "layout": "IPY_MODEL_cfa849b7d9de4510826fda6c506d928d"
          }
        },
        "97693fb687c8418899da26c59eee2b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ebe3a01e76444ffbc5a57aed228ac54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a38724c034c2479dac4d1d659f5238e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ee13f761de4fe986de320730542b55",
            "placeholder": "",
            "style": "IPY_MODEL_07571df3674a45f581cc2b555439c60c",
            "value": "vocab.txt:100%"
          }
        },
        "bb5f890586d544d2958336f144fc8394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd250209a784cf1996ee289d0692aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c892f76d0a344822a0649ad81d4ff920": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a38724c034c2479dac4d1d659f5238e3",
              "IPY_MODEL_f46afb0540384a19b5955715aa49721b",
              "IPY_MODEL_027c3ccd1f534c9198c345413d928c05"
            ],
            "layout": "IPY_MODEL_3abb2234b9474682bf3764d555b82660"
          }
        },
        "c8fb9f2f59f54f0691a6e0a28869e556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f6d391d4e2a434d9d05ff73ccca43f3",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cbcbb0bc28a41d68804f74901aaf677",
            "value": 48
          }
        },
        "cea9deb568a043819b2dd05cac85df34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa849b7d9de4510826fda6c506d928d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ed21b8f9644662abdf1d526e49a3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d9191237164b988943d82af185189b",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e10572a311264ae4a1269e2684f65846",
            "value": 440449768
          }
        },
        "d851b8c85fe3472b8e123302ac79213d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10572a311264ae4a1269e2684f65846": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f46afb0540384a19b5955715aa49721b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81b3033d583413d96e8b51ddc5a445d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b8ba04d004b4624950c03311629d1ac",
            "value": 231508
          }
        },
        "f5fecc3c4e154dbfb7b9ac1d776fdd7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d39cd82cbd24b78b7bd3181e227d30b",
            "placeholder": "",
            "style": "IPY_MODEL_5e33156da7be42f89533f6b650f702ad",
            "value": "config.json:100%"
          }
        },
        "f81b3033d583413d96e8b51ddc5a445d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f84b04465d104da783ed7205e76e2bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff88f9f79cb415aaa45deb82d14ef4f",
            "placeholder": "",
            "style": "IPY_MODEL_490ea7edeb3644c98f5d80e4c97c8eeb",
            "value": "tokenizer.json:100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
